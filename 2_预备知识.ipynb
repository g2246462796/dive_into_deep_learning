{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIknupKSF1tE/BJ2bwtisV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g2246462796/dive_into_deep_learning/blob/main/2_%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# google云盘操作"
      ],
      "metadata": {
        "id": "y5OwbAcGkaMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP5aR4ymjw8g",
        "outputId": "f23da76d-7a77-4d40-92ec-c252ccb5341c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0L1EJuukuHn",
        "outputId": "fc184dab-99bc-4ea7-f6a6-3430106b580c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1数据操作"
      ],
      "metadata": {
        "id": "MDgFJk6KbN5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1 入门"
      ],
      "metadata": {
        "id": "d09yTqShbT8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw4BNLRPfCvh"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "arrange创建一维向量,从0到11"
      ],
      "metadata": {
        "id": "Qc_4C7AVlhJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(12)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y3CqDiXlwOf",
        "outputId": "c012dad2-e29d-4cbc-e500-ca2d2974575d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "shape属性访问张量的形状(沿每个轴的元素个数)"
      ],
      "metadata": {
        "id": "b6dU0LSNmDgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZoZ4E4LmIBf",
        "outputId": "68167f35-6d5d-4e38-98f3-3e4d917e2051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "numel显示元素个数(即各个维度的乘积)"
      ],
      "metadata": {
        "id": "dah5BO4xmpXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.numel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkmPX1ubmtw2",
        "outputId": "e451e6d2-6329-41eb-e660-71355aaafa40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reshape改变形状但是不改变元素数量和元素值,可以只指定一个维度,另一个维度-1,会自动计算\n",
        "\n"
      ],
      "metadata": {
        "id": "2cp593Fum83t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=x.reshape(3,4)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmcbX_conPGo",
        "outputId": "97f550fd-a361-418e-ead8-f6725d24545e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  3],\n",
              "        [ 4,  5,  6,  7],\n",
              "        [ 8,  9, 10, 11]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "初始化矩阵,用特定函数来指定张量,如zeros,ones,randn(均值为0,标准差为1的标准高斯分布,即正态分布)"
      ],
      "metadata": {
        "id": "YWM2ESbZnSkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros((2,3,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3WWT8okn3Nh",
        "outputId": "f5f70bdd-76ab-4fac-ec03-04c5f7a21a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones((2,3,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An4fWIB-oPdW",
        "outputId": "2d4e204b-7073-495f-b9fa-43fe10f9b08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(3,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1iTPHaLoTIS",
        "outputId": "063588c6-10ee-425f-86c7-accf8ec19ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5102, -0.9889, -0.7005,  2.3983],\n",
              "        [-0.7397, -1.0218, -0.1483,  0.7696],\n",
              "        [-1.4915,  0.0194, -2.5886, -0.2799]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "也可以直接给元素赋值来实现"
      ],
      "metadata": {
        "id": "D6yUydx6onWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-riQlHKNorCm",
        "outputId": "8f32a2f9-3c02-43c1-d0f3-0b4c84eea3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.2运算符"
      ],
      "metadata": {
        "id": "PRCfFsc-pGbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "一些基础运算,加减乘除幂等"
      ],
      "metadata": {
        "id": "ibUExkrwbHHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0,2,4,8])\n",
        "y = torch.tensor([2,2,2,2])\n",
        "x + y, x - y, x * y, x / y, x ** y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Qu_j_kbDpO",
        "outputId": "ce548ef7-9bd0-4d44-b40b-b4dfca687da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "求指数"
      ],
      "metadata": {
        "id": "zvUcnsexcJkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRn8CB4UcK3c",
        "outputId": "eb93e9f6-e870-4e37-87a4-21af1bb610ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "改变维度以及按行和列分别连接两个矩阵"
      ],
      "metadata": {
        "id": "eOyocDZMcNV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
        "Y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
        "torch.cat((X,Y),dim=0),torch.cat((X,Y),dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2FR7bAvcVs-",
        "outputId": "c5eea76b-e670-4cd2-aeca-180e9f27c006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [ 2.,  1.,  4.,  3.],\n",
              "         [ 1.,  2.,  3.,  4.],\n",
              "         [ 4.,  3.,  2.,  1.]]),\n",
              " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "逻辑运算"
      ],
      "metadata": {
        "id": "BEHPYw-adIem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X == Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cekbqPNVdEvO",
        "outputId": "aa7b3815-2e7c-4b5a-d446-35c5f6497a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True, False,  True],\n",
              "        [False, False, False, False],\n",
              "        [False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对所有元素求和"
      ],
      "metadata": {
        "id": "T7XapUuGdKdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_3hy969dNCh",
        "outputId": "de49e159-a9b3-4ff9-c2fe-98acfc6d0331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(66.)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.3 广播机制"
      ],
      "metadata": {
        "id": "uMNghSI9dQYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于形状不相同的张量,存在广播机制:\n",
        "1.适当复制元素来扩展一个或者两个数组,以便在转换之后,两个张量具有相同的形状;\n",
        "2.对生成的数组执行按元素操作。\n",
        "大多数情况,沿着数组中长度为1的轴进行广播"
      ],
      "metadata": {
        "id": "X_eqRvGxdWyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape((3,1))\n",
        "b = torch.arange(2).reshape((1,2))\n",
        "a,b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaRhBY-fdyEL",
        "outputId": "59c0cab8-4ee8-45ea-a8f2-c408fed96b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]),\n",
              " tensor([[0, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "形状不匹配,矩阵a将复制列,矩阵b将复制行,然后按元素相加"
      ],
      "metadata": {
        "id": "lSj-23UXd_5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OnxyOmXeHn-",
        "outputId": "8b80ef18-6eee-4697-82d2-5c5a6dc973ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.4 索引和切片"
      ],
      "metadata": {
        "id": "Q4HKo2paeKQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "和数组一样,张量中的元素可以通过索引来访问"
      ],
      "metadata": {
        "id": "5P3V6GNeePXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[-1],X[1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtXd2XB_eVqV",
        "outputId": "a4fc5af9-e3bf-45d9-f410-a350f0228627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]),\n",
              " tensor([[ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "指定索引来将元素写入矩阵"
      ],
      "metadata": {
        "id": "9ifyqHVXeoMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[1,2] = 9\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSceLkHFer7l",
        "outputId": "9c4d3c04-53ce-4359-ac48-de390c9cc1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  9.,  7.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "赋值多个元素"
      ],
      "metadata": {
        "id": "HorF089GewST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:2,:] = 12\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaiZLNSTfvC5",
        "outputId": "24e4db66-0655-44a7-cb4d-fedf22847a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12., 12., 12., 12.],\n",
              "        [12., 12., 12., 12.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.5 节省内存"
      ],
      "metadata": {
        "id": "XskJ668Rf0Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行一些操作可能会导致为新结果分配内存,我们将取消引用张量,二十指向新分配的内存处的张量"
      ],
      "metadata": {
        "id": "gyjwSzHnf-56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(Y)\n",
        "Y = Y + X\n",
        "id(Y) == before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXHBHY2wgMQj",
        "outputId": "b37fbf91-5659-4a25-996b-392897a3dff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "为避免分配不必要的内存,我们使用切片表示法,将操作的结果分配给先前分配的数组"
      ],
      "metadata": {
        "id": "3iCTQY3kgWHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = torch.zeros_like(Y)\n",
        "print('id(Z):',id(Z))\n",
        "Z[:] = X + Y\n",
        "print('id(Z)',id(Z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AKAT_G-ghHZ",
        "outputId": "b3682af7-18d5-4131-c67a-5debb6f18444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id(Z): 134424476005088\n",
            "id(Z) 134424476005088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果后续计算没有重复使用X,也可以切片法或+=来减少内存开销"
      ],
      "metadata": {
        "id": "Tm25_a9agwJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(X)\n",
        "X += Y\n",
        "id(X) == before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHrPw2sOg6mi",
        "outputId": "f4fc715f-d534-497c-855d-669dedf258b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.6 转换为其他python对象"
      ],
      "metadata": {
        "id": "iEBvYfdmhBEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "张量转化很容易，共享底层内存，就地操作更改一个，另一个也会同时改变"
      ],
      "metadata": {
        "id": "vymcGI2ohG_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = X.numpy()\n",
        "B = torch.tensor(A)\n",
        "type(A),type(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVTC8akRhROa",
        "outputId": "54a5bea1-f2b2-4b60-d8ce-8eacbe09b496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将张量转化为标量，可以调用item函数或Python内置函数"
      ],
      "metadata": {
        "id": "m-ciD_5ghZLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([3.5])\n",
        "a,a.item(),float(a),int(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcTj2TsxhfBW",
        "outputId": "ebc47f3a-ceb9-4c0e-c95a-b219e662c894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3.5000]), 3.5, 3.5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.7 小结"
      ],
      "metadata": {
        "id": "ZTIiDZcAhnFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "深度学习存储和操作数据的主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转化为其他python对象"
      ],
      "metadata": {
        "id": "hxSW2lPwhsy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.8 练习"
      ],
      "metadata": {
        "id": "0tE52Shjh9bT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.运行本节中的代码。将本节中的条件语句X == Y更改为X < Y或X > Y，然后看看你可以得到什么样的张量。\n",
        "\n",
        "2.用其他形状（例如三维张量）替换广播机制中按元素操作的两个张量。结果是否与预期相同？"
      ],
      "metadata": {
        "id": "l_T-QdigiEWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里只写一下2"
      ],
      "metadata": {
        "id": "yYanoeL_iLwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape((3,1))\n",
        "c = torch.arange(9).reshape((3,3))\n",
        "a,c,a+c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuMXyIh8iN5W",
        "outputId": "1148672e-6dfb-405d-fc93-cd1f84898d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]),\n",
              " tensor([[0, 1, 2],\n",
              "         [3, 4, 5],\n",
              "         [6, 7, 8]]),\n",
              " tensor([[ 0,  1,  2],\n",
              "         [ 4,  5,  6],\n",
              "         [ 8,  9, 10]]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2数据预处理"
      ],
      "metadata": {
        "id": "hLzIlq8Bip4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "现实世界的数据并不是拿来直接用的,而是要经过预处理，通常用pandas软件包来进行数据预处理。"
      ],
      "metadata": {
        "id": "RO5KrMRIiwlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.1 读取数据集"
      ],
      "metadata": {
        "id": "6aHhpQUri7gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
        "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
        "with open(data_file, 'w') as f:\n",
        "    f.write('NumRooms,Alley,Price\\n')  # 列名\n",
        "    f.write('NA,Pave,127500\\n')  # 每行表示一个数据样本\n",
        "    f.write('2,NA,106000\\n')\n",
        "    f.write('4,NA,178100\\n')\n",
        "    f.write('NA,NA,140000\\n')"
      ],
      "metadata": {
        "id": "U3Mie0rRi-QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "要从创建的CSV文件中加载原始数据集，我们导入pandas包并调用read_csv函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。"
      ],
      "metadata": {
        "id": "jl_HNrmDlVe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 如果没有安装pandas，只需取消对以下行的注释来安装pandas\n",
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "print(data)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "jl2TITe8lXUa",
        "outputId": "c82cc777-2b52-4036-fc6a-95ef382a7503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms Alley   Price\n",
            "0       NaN  Pave  127500\n",
            "1       2.0   NaN  106000\n",
            "2       4.0   NaN  178100\n",
            "3       NaN   NaN  140000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   NumRooms Alley   Price\n",
              "0       NaN  Pave  127500\n",
              "1       2.0   NaN  106000\n",
              "2       4.0   NaN  178100\n",
              "3       NaN   NaN  140000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d5689b6-f20e-4b4f-8505-31dc5dea9e3d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumRooms</th>\n",
              "      <th>Alley</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Pave</td>\n",
              "      <td>127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>178100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d5689b6-f20e-4b4f-8505-31dc5dea9e3d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d5689b6-f20e-4b4f-8505-31dc5dea9e3d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d5689b6-f20e-4b4f-8505-31dc5dea9e3d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-96337305-4b96-44d9-83da-fd124f4d1798\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96337305-4b96-44d9-83da-fd124f4d1798')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-96337305-4b96-44d9-83da-fd124f4d1798 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.2 处理缺失值"
      ],
      "metadata": {
        "id": "Cq76VamXlfiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括插值法和删除法， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。\n",
        "\n",
        "\n",
        "通过位置索引iloc，我们将data分成inputs和outputs， 其中前者为data的前两列，而后者为data的最后一列。 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项。"
      ],
      "metadata": {
        "id": "D0GjwH9hlipb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs,outputs = data.iloc[:,0:2],data.iloc[:,2]\n",
        "inputs = inputs.fillna(inputs.mean())\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "5XbMZ_p6lmtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于inputs中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。"
      ],
      "metadata": {
        "id": "BaE1bXg1mhls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZVk64bzmgZe",
        "outputId": "5bbac74b-745b-47ac-b4f8-28f92f67db20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms  Alley_Pave  Alley_nan\n",
            "0       3.0           1          0\n",
            "1       2.0           0          1\n",
            "2       4.0           0          1\n",
            "3       3.0           0          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3 转换为张量格式"
      ],
      "metadata": {
        "id": "Yq43_p8-mLZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在inputs和outputs中的所有条目都是数值类型，它们可以转换为张量格式。 当数据采用张量格式后，可以通过在 2.1节中引入的那些张量函数来进一步操作。"
      ],
      "metadata": {
        "id": "nEPF3jFlmRO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
        "y = torch.tensor(outputs.to_numpy(dtype=float))\n",
        "X, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjxgtcK_mTnK",
        "outputId": "18ba2ddc-c61c-436c-a442-4e4c94362705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[3., 1., 0.],\n",
              "         [2., 0., 1.],\n",
              "         [4., 0., 1.],\n",
              "         [3., 0., 1.]], dtype=torch.float64),\n",
              " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.4 小结"
      ],
      "metadata": {
        "id": "e2y5bswbmuS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pandas软件包是Python中常用的数据分析工具,可以与张量兼容\n",
        "\n",
        "用pandas处理缺失数据时,根据情况选择插值法和删除法"
      ],
      "metadata": {
        "id": "sB913T_bmzT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.5 练习"
      ],
      "metadata": {
        "id": "cLCD3AvMnEf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "创建包含更多行和列的原始数据集。\n",
        "\n",
        "删除缺失值最多的列。\n",
        "\n",
        "将预处理后的数据集转换为张量格式。"
      ],
      "metadata": {
        "id": "xW7Z2l67nG_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3线性代数"
      ],
      "metadata": {
        "id": "EwtUHg6H3npG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "回顾一些基本线性代数内容，并用代码来实现这些计算"
      ],
      "metadata": {
        "id": "vaNlI68-3wJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.1标量"
      ],
      "metadata": {
        "id": "xLu9cwk332Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "数学表示法：小写字母表示标量变量，R表示所有实数标量的空间"
      ],
      "metadata": {
        "id": "fPUdUUHr34je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "x + y, x * y, x / y, x**y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csZQDEny4RnH",
        "outputId": "93a58c8e-3781-44a3-8bc2-3ceb00cd17f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.2 向量"
      ],
      "metadata": {
        "id": "aP-33Qil4cwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "向量可以被视为标量组成的列表，标量值被称为向量的元素或分量。通过一维张量表示向量"
      ],
      "metadata": {
        "id": "t2R2_twn4fKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_78bstp4wvh",
        "outputId": "6fb27166-1180-4f63-8888-e31fa2f18ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过下标访问,默认列向量是向量的默认方向"
      ],
      "metadata": {
        "id": "g02vZcYu43Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaJKsqAl491F",
        "outputId": "ab28d16e-a815-4960-c3ed-86da61275300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2.1 长度、维度和形状"
      ],
      "metadata": {
        "id": "6ANGIRAh4_sX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "向量只是一个数字数字，向量的长度通常称为向量的维度。\n",
        "可以使用len函数来访问张量的长度"
      ],
      "metadata": {
        "id": "8AgEGemU5Gld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-E-Uzam5OI6",
        "outputId": "91989133-8cf5-4a83-9417-90a03bfaa1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "因为只有一个轴，我们也可以通过.shape属性访问向量的长度"
      ],
      "metadata": {
        "id": "qhFYXjuG5Vf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO2JbJa75ajE",
        "outputId": "7e36118f-e459-4f05-e223-c6d40956e844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "请注意，维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。 为了清楚起见，我们在此明确一下： 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。"
      ],
      "metadata": {
        "id": "02Ptus-X5fdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.3 矩阵"
      ],
      "metadata": {
        "id": "mU305btG5hHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "向量将标量从零阶推广到一阶,矩阵将向量从一阶推广到二阶。\n",
        "用函数来实例化张量时，我们可以通过指定两个分量m和n来创建一个形状为m * n的矩阵"
      ],
      "metadata": {
        "id": "yrIzbhjj5oe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20).reshape(5,4)\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPNXNgjM5-ak",
        "outputId": "689d40ac-0ab8-44d4-d334-4bd0e63c36f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  3],\n",
              "        [ 4,  5,  6,  7],\n",
              "        [ 8,  9, 10, 11],\n",
              "        [12, 13, 14, 15],\n",
              "        [16, 17, 18, 19]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "转置"
      ],
      "metadata": {
        "id": "b0lnsvzx6L_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvmVKd4R6NLT",
        "outputId": "4429cff8-e1c4-4111-9be7-699c4618c3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  4,  8, 12, 16],\n",
              "        [ 1,  5,  9, 13, 17],\n",
              "        [ 2,  6, 10, 14, 18],\n",
              "        [ 3,  7, 11, 15, 19]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对称矩阵A等于其转置，这里定义一个对称矩阵B"
      ],
      "metadata": {
        "id": "P0SQTvkj6Shl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])\n",
        "B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMdR0MXc6XSW",
        "outputId": "1c7f2d52-cc54-4933-c064-c5cb21fd4c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [2, 0, 4],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "与他的转置相比较"
      ],
      "metadata": {
        "id": "rIjiTQ866i6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B == B.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M6JlE6i6lMb",
        "outputId": "26dd8fc6-7c51-4025-89a3-167b505b8bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据。 例如，我们矩阵中的行可能对应于不同的房屋（数据样本），而列可能对应于不同的属性。 曾经使用过电子表格软件或已阅读过 2.2节的人，应该对此很熟悉。 因此，尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中， 将每个数据样本作为矩阵中的行向量更为常见。 后面的章节将讲到这点，这种约定将支持常见的深度学习实践。 例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。"
      ],
      "metadata": {
        "id": "iW6sFhwl6nze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.4 张量"
      ],
      "metadata": {
        "id": "piJtcFMR6rkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。"
      ],
      "metadata": {
        "id": "P-66BH3R6vRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(24).reshape(2,3,4)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO88yXw561H3",
        "outputId": "8b65303f-3987-41b3-b36b-7bcf39a9d9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3],\n",
              "         [ 4,  5,  6,  7],\n",
              "         [ 8,  9, 10, 11]],\n",
              "\n",
              "        [[12, 13, 14, 15],\n",
              "         [16, 17, 18, 19],\n",
              "         [20, 21, 22, 23]]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.5 张量算法的基本性质"
      ],
      "metadata": {
        "id": "Vamis_IT66hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。 例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。"
      ],
      "metadata": {
        "id": "NfFAGxp66_sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
        "B = A.clone()\n",
        "A,A+B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymd3PkcY7CBe",
        "outputId": "31b53f4b-7e83-453a-9de5-50d92c9cd835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]),\n",
              " tensor([[ 0.,  2.,  4.,  6.],\n",
              "         [ 8., 10., 12., 14.],\n",
              "         [16., 18., 20., 22.],\n",
              "         [24., 26., 28., 30.],\n",
              "         [32., 34., 36., 38.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "矩阵的积"
      ],
      "metadata": {
        "id": "qmPXxj9W7T1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A * B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FARqAatL7VUo",
        "outputId": "be77f491-289e-454d-fb9c-104b6008ff32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.,   1.,   4.,   9.],\n",
              "        [ 16.,  25.,  36.,  49.],\n",
              "        [ 64.,  81., 100., 121.],\n",
              "        [144., 169., 196., 225.],\n",
              "        [256., 289., 324., 361.]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。"
      ],
      "metadata": {
        "id": "AcSGcH187Z-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2\n",
        "X = torch.arange(24).reshape(2,3,4)\n",
        "a + X, (a * X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW706TWE7bCg",
        "outputId": "2e50112d-fe78-49bc-e7c8-d1f94002dd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 2,  3,  4,  5],\n",
              "          [ 6,  7,  8,  9],\n",
              "          [10, 11, 12, 13]],\n",
              " \n",
              "         [[14, 15, 16, 17],\n",
              "          [18, 19, 20, 21],\n",
              "          [22, 23, 24, 25]]]),\n",
              " torch.Size([2, 3, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.6 降维"
      ],
      "metadata": {
        "id": "Iuq3nFvW7lGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以对任意张量进行的一个有用的操作是计算其元素的和。"
      ],
      "metadata": {
        "id": "XkypzXhF7wR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4,dtype=torch.float32)\n",
        "x,x.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fq2A9vI7ywN",
        "outputId": "75f5f04c-881f-4f70-a19e-267357176034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.]), tensor(6.))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以表示任意形状张量的元素和。"
      ],
      "metadata": {
        "id": "Dt-7SlBE7612"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape,A.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k7YhR8e77QD",
        "outputId": "600408b2-b778-4088-af2b-f5ab86f5aae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), tensor(190.))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。"
      ],
      "metadata": {
        "id": "ouMnnaFz8A1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A_sum_axis0 = A.sum(axis=0)\n",
        "A_sum_axis0,A_sum_axis0.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjUSOWTD8Bvq",
        "outputId": "37fc7543-f244-44ab-c1e4-ff9bf38ad20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "指定axis=1将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。"
      ],
      "metadata": {
        "id": "MJ9_swcU8UJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A_sum_axis1 = A.sum(axis=1)\n",
        "A_sum_axis1, A_sum_axis1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsVTIalQ8U-v",
        "outputId": "9a7aa40e-250b-4425-fa36-83b3b4ef946c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。"
      ],
      "metadata": {
        "id": "xT_f1MDG8Xy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.sum(axis=[0, 1])  # 结果和A.sum()相同"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTaI2e-T8YbV",
        "outputId": "d2eebff4-a578-413c-f05e-182429cd3ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(190.)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "一个与求和相关的量是平均值（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。"
      ],
      "metadata": {
        "id": "K7bJLZPk8eiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(), A.sum() / A.numel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPO5NhdM8fJs",
        "outputId": "80957c01-3009-44d6-ad81-809c519e504e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(9.5000), tensor(9.5000))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A.mean(), A.sum() / A.numel()"
      ],
      "metadata": {
        "id": "-PRzKsEc8hPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV-2nCZx8hmn",
        "outputId": "9db8583b-0fff-4d3f-dbf1-4ac6c7e42a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.6.1 非降维求和"
      ],
      "metadata": {
        "id": "Wj_pWJ468j95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。"
      ],
      "metadata": {
        "id": "lCvkH19t8o4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_A = A.sum(axis=1, keepdims=True)\n",
        "sum_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_bYdltj8pyV",
        "outputId": "cf375fd4-7c29-4d4e-eb83-832a4e133d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.],\n",
              "        [22.],\n",
              "        [38.],\n",
              "        [54.],\n",
              "        [70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "例如，由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。"
      ],
      "metadata": {
        "id": "-MRPJYkV8wDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A / sum_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6DIVwOY80PH",
        "outputId": "c890badf-0d4d-4f5b-b38e-e7eda66819f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
              "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
              "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
              "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
              "        [0.2286, 0.2429, 0.2571, 0.2714]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果我们想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），可以调用cumsum函数。 此函数不会沿任何轴降低输入张量的维度。"
      ],
      "metadata": {
        "id": "C7k79xVS83nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.cumsum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSZAnJxU84NS",
        "outputId": "77e8820a-0a9d-4926-8852-975828ab6bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  6.,  8., 10.],\n",
              "        [12., 15., 18., 21.],\n",
              "        [24., 28., 32., 36.],\n",
              "        [40., 45., 50., 55.]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.7 点积"
      ],
      "metadata": {
        "id": "qQnmXT6F8-q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.ones(4, dtype = torch.float32)\n",
        "x, y, torch.dot(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWAEHEJg9CB8",
        "outputId": "267742be-13f3-48c5-f080-49c1c384af67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意，我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积"
      ],
      "metadata": {
        "id": "aIqCs9h69UBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(x * y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4CrWF2Z9W5C",
        "outputId": "789ce3c6-3f56-413b-fba2-afec35f35dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "点积在很多场合都很有用。例如向量和权重点积来获得加权平均"
      ],
      "metadata": {
        "id": "OFEm6iYT9b2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.8 矩阵-向量积"
      ],
      "metadata": {
        "id": "I_aotXmm9jBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在代码中使用张量表示矩阵-向量积，我们使用mv函数。 当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。"
      ],
      "metadata": {
        "id": "hFR9tZW79uCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, x.shape, torch.mv(A, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RdqfiHr9n7t",
        "outputId": "d3b6c192-c16a-408e-aacc-23381bb71885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.9 矩阵-矩阵乘法"
      ],
      "metadata": {
        "id": "LD_4aZO190wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.ones(4, 3)\n",
        "torch.mm(A, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_AqoYut97ox",
        "outputId": "dc5d1b71-bfd0-45d4-cf54-fa58c4ee0662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.,  6.,  6.],\n",
              "        [22., 22., 22.],\n",
              "        [38., 38., 38.],\n",
              "        [54., 54., 54.],\n",
              "        [70., 70., 70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.10 范数"
      ],
      "metadata": {
        "id": "LA2O__a--Bdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "线性代数中最有用的一些运算符是范数（norm）。 非正式地说，向量的范数是表示一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。\n",
        "在线性代数中，向量范数是将向量映射到标量的函数f。\n",
        "三条性质:\n",
        "1.缩放所有元素，范数也会按常数因子的绝对值缩放\n",
        "2.三角不等式\n",
        "3.范数非负\n",
        "范数听起来很像距离的度量。"
      ],
      "metadata": {
        "id": "TouDzKmI-GMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算L2范数"
      ],
      "metadata": {
        "id": "1FQRgWXY-fYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u = torch.tensor([3.0, -4.0])\n",
        "torch.norm(u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RNh5bjD-hHU",
        "outputId": "cc3081b2-d63c-44ff-d894-286c666a823b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算L1范数(向量元素的绝对值之和)"
      ],
      "metadata": {
        "id": "Mvg1L9oD-tWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.abs(u).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy9PwpMD-qZq",
        "outputId": "43ea7aeb-2ebe-4e04-97e5-28697c767e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frobenius范数,L1和L2都是此范数的特例"
      ],
      "metadata": {
        "id": "qceAQ1EV-xMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(torch.ones((4, 9)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHd5TiDZ-6Xy",
        "outputId": "0aa8fd80-e0cc-438c-c710-2562518ef58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.10.1 范数和目标"
      ],
      "metadata": {
        "id": "EOb500cA-90n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。"
      ],
      "metadata": {
        "id": "ongs1d3u_BHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.11 关于线性代数的更多信息"
      ],
      "metadata": {
        "id": "oGZT7GVE_E2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。"
      ],
      "metadata": {
        "id": "OAILZTLt_JQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.12 小结"
      ],
      "metadata": {
        "id": "d92K_Knd_LO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "标量、向量、矩阵和张量是线性代数中的基本数学对象。\n",
        "\n",
        "向量泛化自标量，矩阵泛化自向量。\n",
        "\n",
        "标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。\n",
        "\n",
        "一个张量可以通过sum和mean沿指定的轴降低维度。\n",
        "\n",
        "两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。\n",
        "\n",
        "在深度学习中，我们经常使用范数，如L1范数、L2范数和Frobenius范数。\n",
        "\n",
        "我们可以对标量、向量、矩阵和张量执行各种操作。"
      ],
      "metadata": {
        "id": "7bGA2njF_N_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.13 练习"
      ],
      "metadata": {
        "id": "fu6Svhx-_T5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 微积分"
      ],
      "metadata": {
        "id": "ZEOB5yR1iOqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个损失函数（loss function）， 即一个衡量“模型有多糟糕”这个问题的分数。 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。"
      ],
      "metadata": {
        "id": "ws7nbK1riSb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "因此，我们可以将拟合模型的任务分解为两个关键问题：\n",
        "\n",
        "优化（optimization）：用模型拟合观测数据的过程；\n",
        "\n",
        "泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。"
      ],
      "metadata": {
        "id": "noefusYgid0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.1 导数和微分"
      ],
      "metadata": {
        "id": "wI0h0S4XimhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.14\n",
        "%matplotlib inline\n",
        "from matplotlib_inline import backend_inline\n",
        "import numpy as np\n",
        "from d2l import torch as d2l\n",
        "def f(x):\n",
        "  return 3 * x**2 - 4 * x\n",
        "def numerical_lim(f,x,h):\n",
        "  return (f(x+h)-f(x))/h\n",
        "h = 0.1\n",
        "for i in range(5):\n",
        "  print(f'h={h:.5f},numerical limit={numerical_lim(f,1,h):.5f}')\n",
        "  h *= 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc6r4kxWiutc",
        "outputId": "2147f9ae-7a43-4cef-aefa-8dea2b68f6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l==0.14\n",
            "  Downloading d2l-0.14.0-py3-none-any.whl (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter (from d2l==0.14)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from d2l==0.14) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from d2l==0.14) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from d2l==0.14) (1.5.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->d2l==0.14) (6.5.5)\n",
            "Collecting qtconsole (from jupyter->d2l==0.14)\n",
            "  Downloading qtconsole-5.4.4-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->d2l==0.14) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->d2l==0.14) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->d2l==0.14) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->d2l==0.14) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->d2l==0.14) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->d2l==0.14) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->d2l==0.14) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->d2l==0.14) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->d2l==0.14) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->d2l==0.14) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->d2l==0.14) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->d2l==0.14) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->d2l==0.14) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->d2l==0.14) (3.0.9)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->d2l==0.14) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->d2l==0.14) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->d2l==0.14) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->d2l==0.14) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->d2l==0.14) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->d2l==0.14) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->d2l==0.14) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->d2l==0.14) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->d2l==0.14) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->d2l==0.14) (1.0.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter->d2l==0.14)\n",
            "  Downloading QtPy-2.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.4/93.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==0.14) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter->d2l==0.14)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==0.14) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==0.14) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==0.14) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==0.14) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==0.14) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->d2l==0.14) (3.11.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->d2l==0.14) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->d2l==0.14) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->d2l==0.14) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->d2l==0.14) (4.19.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->d2l==0.14) (0.2.8)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->d2l==0.14) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->d2l==0.14) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->d2l==0.14) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->d2l==0.14) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->d2l==0.14) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==0.14) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==0.14) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==0.14) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==0.14) (0.10.6)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l==0.14) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l==0.14) (1.6.4)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==0.14) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l==0.14) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l==0.14) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->d2l==0.14) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==0.14) (2.21)\n",
            "Installing collected packages: qtpy, jedi, qtconsole, jupyter, d2l\n",
            "Successfully installed d2l-0.14.0 jedi-0.19.1 jupyter-1.0.0 qtconsole-5.4.4 qtpy-2.4.0\n",
            "h=0.10000,numerical limit=2.30000\n",
            "h=0.01000,numerical limit=2.03000\n",
            "h=0.00100,numerical limit=2.00300\n",
            "h=0.00010,numerical limit=2.00030\n",
            "h=0.00001,numerical limit=2.00003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了对导数的这种解释进行可视化，我们将使用matplotlib， 这是一个Python中流行的绘图库。 要配置matplotlib生成图形的属性，我们需要定义几个函数。 在下面，use_svg_display函数指定matplotlib软件包输出svg图表以获得更清晰的图像。\n",
        "\n",
        "注意，注释#@save是一个特殊的标记，会将对应的函数、类或语句保存在d2l包中。 因此，以后无须重新定义就可以直接调用它们（例如，d2l.use_svg_display()）。"
      ],
      "metadata": {
        "id": "7Wedq_yFklUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(0,3,0.1)\n",
        "d2l.plot(x,[f(x),2 * x - 3],'x','f(x)',legend=['f(x)','Tangent line (x=1)'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ZQlrh5Pgksum",
        "outputId": "20452d3c-c755-4b1e-fac8-b36eded0bddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"183.35625pt\" viewBox=\"0 0 243.529359 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-10-22T13:33:19.297639</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 243.529359 183.35625 \nL 243.529359 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 49.480398 145.8 \nL 49.480398 7.2 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m09da447ee1\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m09da447ee1\" x=\"49.480398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 110.702968 145.8 \nL 110.702968 7.2 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m09da447ee1\" x=\"110.702968\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 171.925539 145.8 \nL 171.925539 7.2 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m09da447ee1\" x=\"171.925539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 233.148109 145.8 \nL 233.148109 7.2 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m09da447ee1\" x=\"233.148109\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 40.603125 116.769994 \nL 235.903125 116.769994 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"mb1777aa37e\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mb1777aa37e\" x=\"40.603125\" y=\"116.769994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 120.569213) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 40.603125 78.886651 \nL 235.903125 78.886651 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mb1777aa37e\" x=\"40.603125\" y=\"78.886651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 82.685869) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 40.603125 41.003307 \nL 235.903125 41.003307 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mb1777aa37e\" x=\"40.603125\" y=\"41.003307\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.802526) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 85.121094) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 49.480398 116.769994 \nL 55.602655 119.573361 \nL 61.724912 121.922129 \nL 67.847169 123.816296 \nL 73.969426 125.255863 \nL 80.091683 126.24083 \nL 86.21394 126.771197 \nL 92.336197 126.846963 \nL 98.458454 126.46813 \nL 104.580711 125.634696 \nL 110.702968 124.346663 \nL 116.825225 122.604029 \nL 122.947482 120.406795 \nL 129.069739 117.754961 \nL 135.191996 114.648527 \nL 141.314254 111.087492 \nL 147.436511 107.071858 \nL 153.558768 102.601624 \nL 159.681025 97.676789 \nL 165.803282 92.297354 \nL 171.925539 86.463319 \nL 178.047796 80.174684 \nL 184.170053 73.431449 \nL 190.29231 66.233614 \nL 196.414567 58.581179 \nL 202.536824 50.474143 \nL 208.659081 41.912508 \nL 214.781338 32.896272 \nL 220.903595 23.425436 \nL 227.025852 13.5 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.480398 139.5 \nL 55.602655 137.984666 \nL 61.724912 136.469333 \nL 67.847169 134.953999 \nL 73.969426 133.438665 \nL 80.091683 131.923331 \nL 86.21394 130.407998 \nL 92.336197 128.892664 \nL 98.458454 127.37733 \nL 104.580711 125.861996 \nL 110.702968 124.346663 \nL 116.825225 122.831329 \nL 122.947482 121.315995 \nL 129.069739 119.800661 \nL 135.191996 118.285328 \nL 141.314254 116.769994 \nL 147.436511 115.25466 \nL 153.558768 113.739327 \nL 159.681025 112.223993 \nL 165.803282 110.708659 \nL 171.925539 109.193325 \nL 178.047796 107.677992 \nL 184.170053 106.162658 \nL 190.29231 104.647324 \nL 196.414567 103.13199 \nL 202.536824 101.616657 \nL 208.659081 100.101323 \nL 214.781338 98.585989 \nL 220.903595 97.070655 \nL 227.025852 95.555322 \n\" clip-path=\"url(#p6f19b6f43c)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 145.8 \nL 40.603125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 145.8 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 145.8 \nL 235.903125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298438 \nL 59.603125 20.298438 \nL 69.603125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798438) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 49.603125 34.976562 \nL 59.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6f19b6f43c\">\n   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.2 偏导数"
      ],
      "metadata": {
        "id": "uJwJtzfVlyiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "到目前为止，我们只讨论了仅含一个变量的函数的微分。 在深度学习中，函数通常依赖于许多变量。 因此，我们需要将微分的思想推广到多元函数（multivariate function）上。"
      ],
      "metadata": {
        "id": "7yrifodHl1eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.3 梯度"
      ],
      "metadata": {
        "id": "N_in-g6hl5B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）向量。"
      ],
      "metadata": {
        "id": "T3WjEY7Jl79q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.4 链式法则"
      ],
      "metadata": {
        "id": "mu5-yA_pmDAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "然而，上面方法可能很难找到梯度。 这是因为在深度学习中，多元函数通常是复合（composite）的， 所以难以应用上述任何规则来微分这些函数。 幸运的是，链式法则可以被用来微分复合函数。"
      ],
      "metadata": {
        "id": "XHSI_AcHmF-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4.5 小结"
      ],
      "metadata": {
        "id": "pNCliLvEmIh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。\n",
        "\n",
        "导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。\n",
        "\n",
        "梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。\n",
        "\n",
        "链式法则可以用来微分复合函数。"
      ],
      "metadata": {
        "id": "CML9szUXmK3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 自动微分"
      ],
      "metadata": {
        "id": "LjimHb45mPq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个计算图（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。"
      ],
      "metadata": {
        "id": "kEJtmOCAmahT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.1 一个简单的例子"
      ],
      "metadata": {
        "id": "VYeRl171mdSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "作为一个演示例子，假设我们想对函数y=2xTx\n",
        "关于列向量x\n",
        "求导。 首先，我们创建变量x并为其分配一个初始值。"
      ],
      "metadata": {
        "id": "VT4JX9bQmhWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.arange(4.0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2L0EnoBmm3u",
        "outputId": "e9c2edbc-6bca-4af4-cd28-07e646149d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在我们计算y\n",
        "关于x\n",
        "的梯度之前，需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量x\n",
        "的梯度是向量，并且与x\n",
        "具有相同的形状。"
      ],
      "metadata": {
        "id": "yeCN01pomsr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
        "x.grad  # 默认值是None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTsc_X5JmyZN",
        "outputId": "740fdf5f-bdef-49c3-9c51-aa357e3d3fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算y"
      ],
      "metadata": {
        "id": "HNWN4ezwm5pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = 2 * torch.dot(x,x)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lCK6PfBm6sK",
        "outputId": "097eba73-2a3e-472a-83e9-ea87698f4ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。"
      ],
      "metadata": {
        "id": "DTETdPbCnDQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrGhqziynD7q",
        "outputId": "c8f37323-4e8e-4b0a-e305-de40cfa8ee02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "快速验证这个梯度是否计算正确。"
      ],
      "metadata": {
        "id": "gXiG6Xg6nK6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad == 4 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7D6gFSknLTQ",
        "outputId": "c4ba3587-2584-4884-b249-995e9e102395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在计算x的另一个函数"
      ],
      "metadata": {
        "id": "UHhx_EdtnP6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
        "x.grad.zero_()\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yew7qmD3nQ7a",
        "outputId": "79e4657f-90cc-47f7-dc4b-9fc2e702e588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.2 非标量变量的反向传播"
      ],
      "metadata": {
        "id": "FMCDxEOfnuVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。"
      ],
      "metadata": {
        "id": "dsWkS2Vvn2vB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。"
      ],
      "metadata": {
        "id": "6tJ1eo69n6IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
        "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "# 等价于y.backward(torch.ones(len(x)))\n",
        "y.sum().backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CsMZwEHn_L7",
        "outputId": "e17d1b63-8d12-49ed-ae32-2065b72072dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.3 分离计算"
      ],
      "metadata": {
        "id": "_51rGOD1oSAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。"
      ],
      "metadata": {
        "id": "24Eiv1RQoVFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。"
      ],
      "metadata": {
        "id": "yjkncj2IoZZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5XF23Hooe_Q",
        "outputId": "972ff28c-7c43-40fb-bd81-4e3e11f2f022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。"
      ],
      "metadata": {
        "id": "DBwR48Mvon7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJdnahfqopZR",
        "outputId": "004e6e34-50e7-4de0-9f67-61e04699764e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.4 Python控制流的梯度计算"
      ],
      "metadata": {
        "id": "fK1b7arqo0Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。"
      ],
      "metadata": {
        "id": "8uXZKkSao6Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "metadata": {
        "id": "ILmkSzbfo9ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ],
      "metadata": {
        "id": "zUC08EnppAo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a，因此可以用d/a验证梯度是否正确。"
      ],
      "metadata": {
        "id": "2wFaaGcvpGmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad == d / a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uopkEzN2pQWV",
        "outputId": "13e7a951-13cd-463e-accc-e2a49059db9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5.5 小结"
      ],
      "metadata": {
        "id": "9Kcq1uc5pTbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。"
      ],
      "metadata": {
        "id": "p04EYMPepVkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.6 概率"
      ],
      "metadata": {
        "id": "eJCWk1nJpbc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "简单地说，机器学习就是做出预测。\n",
        " 因此，概率是一种灵活的语言，用于说明我们的确定程度，并且它可以有效地应用于广泛的领域中。"
      ],
      "metadata": {
        "id": "aZ2FrJKDpd2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6.1 基本概率论"
      ],
      "metadata": {
        "id": "5z0oNo9Fp0jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数， 即此事件（event）概率的估计值。 大数定律（law of large numbers）告诉我们： 随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。 让我们用代码试一试！"
      ],
      "metadata": {
        "id": "3DoE_-ytp5d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.distributions import multinomial\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "QkiVQiKwp8B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "在统计学中，我们把从概率分布中抽取样本的过程称为抽样（sampling）。 笼统来说，可以把分布（distribution）看作对事件的概率分配， 稍后我们将给出的更正式定义。 将概率分配给一些离散选择的分布称为多项分布（multinomial distribution）。\n",
        "\n",
        "为了抽取一个样本，即掷骰子，我们只需传入一个概率向量。 输出是另一个相同长度的向量：它在索引i\n",
        "处的值是采样结果中\n",
        "i出现的次数。"
      ],
      "metadata": {
        "id": "5b3ZTgQGp-k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fair_probs = torch.ones([6]) / 6\n",
        "multinomial.Multinomial(1,fair_probs).sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siz6jO8qqEpl",
        "outputId": "60bb98fb-0880-42e1-d541-70f89ea4c101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 1., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在估计一个骰子的公平性时，我们希望从同一分布中生成多个样本。 如果用Python的for循环来完成这个任务，速度会慢得惊人。 因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。"
      ],
      "metadata": {
        "id": "DUe4dT0EqiNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multinomial.Multinomial(10, fair_probs).sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDCAITcaqkDH",
        "outputId": "b2f6d916-2510-41bd-d519-b9c879e91e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 4., 1., 1., 1., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在我们知道如何对骰子进行采样，我们可以模拟1000次投掷。 然后，我们可以统计1000次投掷后，每个数字被投中了多少次。 具体来说，我们计算相对频率，以作为真实概率的估计。"
      ],
      "metadata": {
        "id": "iBUMmxlpquKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 将结果存储为32位浮点数以进行除法\n",
        "counts = multinomial.Multinomial(1000, fair_probs).sample()\n",
        "counts / 1000  # 相对频率作为估计值"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWLkJmncqya8",
        "outputId": "55535e88-ff4f-4810-b276-19681b98c782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1730, 0.1640, 0.1670, 0.1610, 0.1720, 0.1630])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "因为我们是从一个公平的骰子中生成的数据，我们知道每个结果都有真实的概率六分之一所以上面输出的估计值看起来不错。"
      ],
      "metadata": {
        "id": "oomwTKYgq3hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们也可以看到这些概率如何随着时间的推移收敛到真实概率。 让我们进行500组实验，每组抽取10个样本"
      ],
      "metadata": {
        "id": "B0WP3mejq7p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = multinomial.Multinomial(10, fair_probs).sample((500,))\n",
        "cum_counts = counts.cumsum(dim=0)\n",
        "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
        "\n",
        "d2l.set_figsize((6, 4.5))\n",
        "for i in range(6):\n",
        "    d2l.plt.plot(estimates[:, i].numpy(),\n",
        "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
        "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
        "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
        "d2l.plt.gca().set_ylabel('Estimated probability')\n",
        "d2l.plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "klI4MbHxq93-",
        "outputId": "cdfc793f-c9b0-4ecd-fd20-b0ddb195eb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x450 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"392.14375pt\" height=\"294.23625pt\" viewBox=\"0 0 392.14375 294.23625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-10-22T13:56:26.073458</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 294.23625 \nL 392.14375 294.23625 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 256.68 \nL 384.94375 256.68 \nL 384.94375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"md70ccaee7e\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#md70ccaee7e\" x=\"65.361932\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#md70ccaee7e\" x=\"126.356649\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(116.812899 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#md70ccaee7e\" x=\"187.351365\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(177.807615 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#md70ccaee7e\" x=\"248.346082\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(238.802332 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#md70ccaee7e\" x=\"309.340799\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(299.797049 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#md70ccaee7e\" x=\"370.335515\" y=\"256.68\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(360.791765 271.278437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Groups of experiments -->\n     <g transform=\"translate(160.397656 284.956562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \nL 3809 1919 \nL 2778 1919 \nL 2778 2438 \nL 4434 2438 \nL 4434 434 \nQ 4069 175 3628 42 \nQ 3188 -91 2688 -91 \nQ 1594 -91 976 548 \nQ 359 1188 359 2328 \nQ 359 3472 976 4111 \nQ 1594 4750 2688 4750 \nQ 3144 4750 3555 4637 \nQ 3966 4525 4313 4306 \nL 4313 3634 \nQ 3963 3931 3569 4081 \nQ 3175 4231 2741 4231 \nQ 1884 4231 1454 3753 \nQ 1025 3275 1025 2328 \nQ 1025 1384 1454 906 \nQ 1884 428 2741 428 \nQ 3075 428 3337 486 \nQ 3600 544 3809 666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-47\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"116.353516\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"177.535156\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"240.914062\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"304.390625\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"356.490234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"388.277344\"/>\n      <use xlink:href=\"#DejaVuSans-66\" x=\"449.458984\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"484.664062\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"516.451172\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"576.224609\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"635.404297\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"698.880859\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"760.404297\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"801.517578\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"829.300781\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"926.712891\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"988.236328\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"1051.615234\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"1090.824219\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m916b1c4ae6\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"245.34\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.05 -->\n      <g transform=\"translate(20.878125 249.139219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"212.940001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 216.73922) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"180.540001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 184.33922) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"148.140002\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 151.939221) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"115.740002\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 119.539221) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"83.340003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.30 -->\n      <g transform=\"translate(20.878125 87.139222) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"50.940003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.35 -->\n      <g transform=\"translate(20.878125 54.739222) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m916b1c4ae6\" x=\"50.14375\" y=\"18.540004\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.40 -->\n      <g transform=\"translate(20.878125 22.339223) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Estimated probability -->\n     <g transform=\"translate(14.798438 185.463437) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"154.492188\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"182.275391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"279.6875\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"340.966797\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"380.175781\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"441.699219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"505.175781\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"536.962891\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"600.439453\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"639.302734\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"700.484375\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"763.960938\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"825.240234\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"888.716797\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"916.5\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"944.283203\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"972.066406\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"1011.275391\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 65.361932 18.54 \nL 65.971879 115.740002 \nL 66.581826 169.739998 \nL 67.191773 196.740001 \nL 67.80172 212.94 \nL 69.021615 212.94 \nL 69.631562 221.040002 \nL 70.241509 205.740001 \nL 70.851456 212.94 \nL 72.681298 212.94 \nL 73.291245 208.311429 \nL 73.901192 212.94 \nL 74.511139 200.790002 \nL 75.121086 193.881173 \nL 75.731034 191.339997 \nL 76.340981 182.245268 \nL 76.950928 177.300001 \nL 77.560875 175.911427 \nL 78.170822 180.539998 \nL 78.780769 179.13131 \nL 79.390717 183.240005 \nL 80.000664 181.836 \nL 80.610611 183.032305 \nL 81.220558 181.740001 \nL 82.440452 183.891723 \nL 83.0504 187.020001 \nL 84.270294 180.539998 \nL 85.490188 182.44588 \nL 86.100135 181.465714 \nL 86.710083 182.340002 \nL 87.32003 179.664328 \nL 88.539924 181.370767 \nL 89.759819 186.071709 \nL 90.369766 180.539998 \nL 91.58966 176.12182 \nL 92.199607 178.38 \nL 92.809554 176.313916 \nL 93.419502 177.09319 \nL 94.029449 173.790004 \nL 94.639396 174.588986 \nL 95.249343 174.060004 \nL 95.85929 172.281175 \nL 96.469237 171.816926 \nL 97.079185 172.59283 \nL 98.909026 178.225717 \nL 100.738868 179.990844 \nL 101.348815 179.459999 \nL 101.958762 180.008853 \nL 102.568709 177.404517 \nL 103.178656 177.968569 \nL 103.788603 177.502505 \nL 104.398551 179.044619 \nL 105.618445 176.187761 \nL 106.228392 177.681179 \nL 106.838339 178.192172 \nL 107.448286 177.762859 \nL 108.058234 174.607603 \nL 108.668181 175.140003 \nL 109.278128 174.770141 \nL 109.888075 175.285943 \nL 110.498022 174.060004 \nL 112.937811 172.747596 \nL 113.547758 171.630006 \nL 114.767652 172.637566 \nL 115.3776 172.342413 \nL 115.987547 172.825712 \nL 116.597494 171.772943 \nL 117.207441 172.251627 \nL 117.817388 171.974482 \nL 120.257177 173.775163 \nL 120.867124 173.496521 \nL 121.477071 171.830329 \nL 122.087018 171.578299 \nL 122.696966 172.013685 \nL 123.306913 171.089997 \nL 123.91686 171.521444 \nL 124.526807 172.605306 \nL 126.356649 173.803368 \nL 126.966596 172.281175 \nL 127.576543 172.67592 \nL 128.18649 172.440005 \nL 130.016332 173.575516 \nL 130.626279 172.740006 \nL 131.236226 173.703303 \nL 134.895909 172.369565 \nL 135.505856 171.602072 \nL 136.115803 172.509229 \nL 136.72575 172.302717 \nL 137.945645 172.980005 \nL 138.555592 171.703643 \nL 139.165539 172.041639 \nL 140.385433 171.656135 \nL 140.995381 171.986398 \nL 142.215275 171.610868 \nL 142.825222 171.933754 \nL 143.435169 171.749306 \nL 144.655064 172.378169 \nL 145.265011 173.176368 \nL 145.874958 173.475336 \nL 146.484905 172.319103 \nL 147.704799 172.916469 \nL 148.314747 173.681606 \nL 149.534641 174.24648 \nL 150.144588 173.134288 \nL 151.364482 172.782251 \nL 152.584377 173.339998 \nL 153.194324 173.16621 \nL 154.414218 171.944085 \nL 155.024165 172.221086 \nL 155.634113 171.62457 \nL 156.24406 172.332004 \nL 156.854007 172.171792 \nL 157.463954 171.587366 \nL 158.073901 171.857645 \nL 158.683848 172.545197 \nL 159.293796 172.387747 \nL 159.903743 171.816926 \nL 160.51369 172.078854 \nL 161.123637 171.927342 \nL 161.733584 172.185281 \nL 162.343531 172.845005 \nL 162.953479 173.094042 \nL 163.563426 172.94 \nL 164.173373 173.582942 \nL 164.78332 173.822931 \nL 165.393267 174.452731 \nL 166.003214 174.68458 \nL 166.613162 174.137609 \nL 167.223109 174.368569 \nL 167.833056 174.980235 \nL 168.443003 175.203529 \nL 170.882792 174.581377 \nL 171.492739 175.170863 \nL 172.102686 175.385457 \nL 172.712633 174.499321 \nL 173.32258 174.715286 \nL 173.932528 175.290838 \nL 175.152422 173.558783 \nL 175.762369 173.419119 \nL 176.982263 173.848693 \nL 177.592211 173.709734 \nL 178.202158 173.920649 \nL 178.812105 173.436258 \nL 180.031999 173.168576 \nL 180.641946 173.377898 \nL 181.251894 173.245756 \nL 182.471788 174.328603 \nL 183.081735 174.193613 \nL 183.691682 173.727694 \nL 184.301629 174.258366 \nL 185.521524 173.994546 \nL 186.131471 174.190253 \nL 186.741418 173.735998 \nL 187.961312 173.482578 \nL 188.57126 173.996159 \nL 189.791154 174.376101 \nL 190.401101 174.248739 \nL 191.620995 174.620773 \nL 192.230943 174.494068 \nL 192.84089 174.98572 \nL 194.060784 175.343772 \nL 194.670731 175.824513 \nL 195.890626 175.566979 \nL 198.940361 176.416365 \nL 199.550309 176.288414 \nL 200.160256 175.869732 \nL 200.770203 176.035969 \nL 201.38015 175.622144 \nL 201.990097 176.076003 \nL 203.209992 176.40079 \nL 203.819939 175.992633 \nL 204.429886 176.153974 \nL 205.039833 176.595657 \nL 205.64978 176.752991 \nL 206.869675 176.507382 \nL 207.479622 176.939998 \nL 208.699516 177.245087 \nL 209.309463 177.669119 \nL 209.91941 177.54504 \nL 210.529358 177.693143 \nL 211.139305 178.11 \nL 211.749252 177.985641 \nL 212.359199 177.594546 \nL 212.969146 177.740003 \nL 213.579093 177.618686 \nL 214.189041 178.027345 \nL 214.798988 177.642439 \nL 215.408935 176.9983 \nL 216.628829 177.286994 \nL 217.238776 176.911204 \nL 217.848724 176.796578 \nL 218.458671 176.939998 \nL 219.068618 176.826164 \nL 219.678565 176.458108 \nL 220.288512 176.347064 \nL 220.898459 175.98375 \nL 221.508407 175.371128 \nL 222.118354 175.516749 \nL 222.728301 175.911427 \nL 223.338248 175.555383 \nL 223.948195 175.698619 \nL 225.16809 175.489046 \nL 225.778037 175.630911 \nL 226.387984 175.527168 \nL 226.997931 175.667816 \nL 227.607878 175.322027 \nL 228.217825 175.462386 \nL 228.827773 175.842609 \nL 229.43772 175.740004 \nL 230.047667 176.116384 \nL 230.657614 176.013529 \nL 231.267561 176.148789 \nL 232.487456 175.945097 \nL 233.097403 175.374787 \nL 234.927244 175.778715 \nL 235.537191 175.680002 \nL 236.147139 175.812598 \nL 236.757086 176.174039 \nL 237.97698 175.976623 \nL 238.586927 175.651585 \nL 239.196874 175.555383 \nL 239.806822 175.234081 \nL 240.416769 174.689997 \nL 241.026716 174.373909 \nL 241.636663 174.730349 \nL 242.856557 174.548218 \nL 243.466505 173.79461 \nL 244.076452 174.148163 \nL 245.906293 174.540001 \nL 246.51624 174.016513 \nL 247.126188 174.146695 \nL 248.346082 173.543324 \nL 248.956029 173.88835 \nL 249.565976 173.803368 \nL 250.175923 173.932111 \nL 250.785871 173.635084 \nL 251.395818 173.128233 \nL 252.005765 173.046843 \nL 252.615712 172.755581 \nL 253.835606 172.596779 \nL 254.445554 172.934854 \nL 255.055501 173.063075 \nL 256.275395 172.904331 \nL 256.885342 173.031433 \nL 257.495289 172.747596 \nL 258.105237 172.669972 \nL 258.715184 173.000379 \nL 259.325131 173.125578 \nL 259.935078 173.4525 \nL 261.154972 173.697761 \nL 262.374867 173.540002 \nL 262.984814 173.860618 \nL 263.594761 173.781719 \nL 264.204708 173.901472 \nL 264.814655 173.822931 \nL 265.424603 174.138787 \nL 266.03455 174.256367 \nL 266.644497 173.981694 \nL 267.254444 174.099033 \nL 267.864391 174.021081 \nL 268.474338 173.749584 \nL 269.084286 173.673138 \nL 269.694233 173.982862 \nL 270.30418 173.521607 \nL 270.914127 172.871366 \nL 271.524074 172.798405 \nL 272.134021 172.5353 \nL 272.743969 172.653788 \nL 273.353916 172.582102 \nL 273.963863 172.69977 \nL 276.403652 172.416657 \nL 277.013599 172.533108 \nL 277.623546 172.09186 \nL 278.233493 172.023428 \nL 279.453387 172.624096 \nL 280.063335 172.738297 \nL 280.673282 172.668813 \nL 281.283229 172.782251 \nL 281.893176 172.713037 \nL 282.503123 172.462687 \nL 283.723018 172.327185 \nL 284.332965 172.620002 \nL 284.942912 172.372684 \nL 286.162806 172.5962 \nL 286.772753 172.529014 \nL 287.382701 172.639729 \nL 287.992648 172.572784 \nL 288.602595 172.682785 \nL 289.822489 172.549755 \nL 290.432436 172.308646 \nL 291.652331 172.178715 \nL 292.262278 172.287992 \nL 292.872225 171.703643 \nL 293.482172 171.295205 \nL 294.092119 171.23362 \nL 294.702067 171.344248 \nL 295.312014 171.111433 \nL 296.531908 170.990531 \nL 297.141855 171.100628 \nL 297.751802 171.04053 \nL 298.36175 171.149922 \nL 298.971697 171.089997 \nL 299.581644 171.198704 \nL 300.191591 171.138962 \nL 300.801538 171.414418 \nL 301.411485 171.020416 \nL 302.021433 170.795017 \nL 303.241327 171.010587 \nL 303.851274 171.282855 \nL 304.461221 171.388858 \nL 305.071168 171.658781 \nL 305.681116 171.435188 \nL 306.291063 171.703643 \nL 306.90101 171.807512 \nL 307.510957 171.748041 \nL 308.120904 171.851282 \nL 308.730851 171.792004 \nL 311.17064 170.916238 \nL 312.390534 171.442468 \nL 313.000482 170.74836 \nL 313.610429 170.851765 \nL 314.220376 170.637799 \nL 314.830323 170.582924 \nL 315.44027 170.843654 \nL 316.050217 170.788548 \nL 316.660165 170.576803 \nL 317.880059 170.780968 \nL 318.490006 170.726537 \nL 319.099953 170.206188 \nL 319.7099 170.463451 \nL 320.319848 170.25551 \nL 321.539742 170.458295 \nL 322.149689 170.712517 \nL 324.589478 170.500569 \nL 325.199425 170.599958 \nL 325.809372 170.547477 \nL 326.419319 170.797344 \nL 327.029266 170.895352 \nL 327.639214 171.14325 \nL 328.249161 171.240002 \nL 328.859108 171.186653 \nL 329.469055 171.282855 \nL 330.079002 171.527586 \nL 330.688949 171.325322 \nL 331.298897 171.568836 \nL 331.908844 171.66329 \nL 332.518791 171.609709 \nL 333.128738 171.703643 \nL 333.738685 171.650206 \nL 334.95858 171.836615 \nL 335.568527 171.491356 \nL 336.178474 171.438876 \nL 336.788421 171.531931 \nL 338.008315 171.427502 \nL 338.618263 171.519957 \nL 339.22821 171.755997 \nL 341.058051 172.028739 \nL 341.667998 172.261583 \nL 342.277946 172.208572 \nL 342.887893 172.297899 \nL 343.49784 171.961447 \nL 344.107787 172.050919 \nL 345.937629 171.895313 \nL 346.547576 171.984158 \nL 347.157523 171.932662 \nL 347.76747 172.160688 \nL 348.377417 172.248393 \nL 348.987365 172.474767 \nL 355.696783 171.913582 \nL 356.306731 171.728284 \nL 356.916678 171.814319 \nL 357.526625 171.495006 \nL 358.136572 171.446447 \nL 358.746519 171.532529 \nL 359.356466 171.752424 \nL 359.966414 171.703643 \nL 361.186308 172.140005 \nL 361.796255 172.22378 \nL 363.016149 172.125279 \nL 364.845991 172.374152 \nL 365.455938 172.324993 \nL 366.675832 172.489096 \nL 367.895727 172.912635 \nL 368.505674 172.993011 \nL 369.725568 172.634399 \nL 369.725568 172.634399 \n\" clip-path=\"url(#p723db9eccb)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 65.361932 212.94 \nL 65.971879 180.539998 \nL 66.581826 191.339997 \nL 67.191773 164.340004 \nL 67.80172 161.099997 \nL 68.411668 169.739998 \nL 69.021615 175.911427 \nL 69.631562 172.440005 \nL 70.241509 184.139997 \nL 70.851456 174.060004 \nL 71.461403 177.594546 \nL 72.071351 185.940002 \nL 72.681298 183.032305 \nL 73.291245 189.79714 \nL 73.901192 182.700005 \nL 74.511139 184.590003 \nL 75.121086 178.634115 \nL 75.731034 176.939998 \nL 76.340981 178.834737 \nL 76.950928 177.300001 \nL 77.560875 166.654284 \nL 78.170822 165.812729 \nL 78.780769 167.861742 \nL 79.390717 167.040001 \nL 80.610611 160.601538 \nL 81.220558 160.140002 \nL 81.830505 162.025713 \nL 82.440452 159.31242 \nL 84.270294 158.265 \nL 86.100135 157.397142 \nL 87.32003 160.399458 \nL 87.929977 158.371582 \nL 88.539924 161.432307 \nL 89.149871 162.720005 \nL 89.759819 162.364396 \nL 90.369766 160.482856 \nL 90.979713 160.195814 \nL 92.199607 162.539999 \nL 92.809554 162.226963 \nL 93.419502 160.548517 \nL 94.029449 162.990005 \nL 94.639396 162.686943 \nL 95.249343 161.099997 \nL 95.85929 160.845881 \nL 97.079185 162.811698 \nL 97.689132 161.340005 \nL 98.299079 163.456368 \nL 98.909026 164.340004 \nL 99.518973 166.329477 \nL 100.12892 166.015862 \nL 100.738868 167.909491 \nL 101.958762 169.385905 \nL 102.568709 167.998065 \nL 103.178656 168.711427 \nL 103.788603 170.414998 \nL 104.398551 169.075389 \nL 105.008498 168.758181 \nL 106.228392 166.245886 \nL 106.838339 167.861742 \nL 107.448286 167.58 \nL 108.058234 168.218877 \nL 108.668181 167.040001 \nL 109.278128 166.781096 \nL 109.888075 168.28054 \nL 110.498022 167.148003 \nL 111.107969 168.603155 \nL 114.157705 171.339999 \nL 114.767652 171.05707 \nL 115.3776 169.219523 \nL 116.597494 170.248239 \nL 117.817388 172.719314 \nL 119.037283 172.166964 \nL 119.64723 172.620002 \nL 120.257177 170.926811 \nL 120.867124 171.38348 \nL 121.477071 172.527102 \nL 123.91686 174.193613 \nL 124.526807 172.605306 \nL 125.136754 172.358181 \nL 125.746701 171.467999 \nL 126.966596 172.281175 \nL 127.576543 172.046796 \nL 128.796437 170.357149 \nL 129.406384 170.758866 \nL 130.016332 170.547477 \nL 130.626279 170.940002 \nL 131.236226 169.541839 \nL 131.846173 170.525452 \nL 133.066067 171.282855 \nL 133.676015 172.224958 \nL 134.285962 172.582102 \nL 135.505856 172.160688 \nL 136.115803 169.739998 \nL 136.72575 170.655257 \nL 137.945645 169.199999 \nL 138.555592 169.02595 \nL 139.165539 168.323605 \nL 139.775486 169.21317 \nL 140.385433 168.520645 \nL 141.605328 169.225713 \nL 142.215275 168.549448 \nL 142.825222 169.402504 \nL 143.435169 169.739998 \nL 144.655064 167.431608 \nL 146.484905 168.450446 \nL 147.704799 167.198822 \nL 148.314747 168.005693 \nL 149.534641 168.652236 \nL 150.144588 169.431433 \nL 150.754535 169.280423 \nL 151.364482 169.587888 \nL 151.97443 168.531614 \nL 153.804271 169.444111 \nL 155.024165 169.156219 \nL 155.634113 169.450069 \nL 156.24406 169.308001 \nL 156.854007 168.309537 \nL 157.463954 167.750525 \nL 158.683848 168.337404 \nL 159.903743 169.739998 \nL 160.51369 169.189686 \nL 161.123637 169.876707 \nL 161.733584 169.739998 \nL 162.343531 170.009998 \nL 164.78332 169.476584 \nL 166.003214 170.000246 \nL 166.613162 169.86934 \nL 168.443003 168.342357 \nL 169.05295 168.982103 \nL 169.662897 168.86093 \nL 170.272845 169.115722 \nL 170.882792 168.622756 \nL 171.492739 169.24629 \nL 172.102686 169.126363 \nL 172.712633 167.909491 \nL 173.32258 167.798428 \nL 173.932528 167.32659 \nL 174.542475 167.58 \nL 175.152422 168.188625 \nL 175.762369 167.722416 \nL 177.592211 168.455679 \nL 179.422052 168.131491 \nL 180.031999 168.711427 \nL 180.641946 168.603155 \nL 181.251894 168.835294 \nL 181.861841 169.402504 \nL 182.471788 168.620835 \nL 183.081735 168.849275 \nL 183.691682 168.743079 \nL 184.301629 168.968575 \nL 184.911577 168.862842 \nL 185.521524 169.08546 \nL 186.131471 168.328945 \nL 186.741418 168.228002 \nL 187.961312 168.670698 \nL 188.57126 168.569562 \nL 189.791154 169.002438 \nL 190.401101 169.53029 \nL 191.011048 169.426962 \nL 191.620995 169.013079 \nL 192.230943 169.22325 \nL 192.84089 168.505717 \nL 194.060784 168.313582 \nL 194.670731 168.523097 \nL 195.280678 168.12505 \nL 195.890626 168.634884 \nL 196.500573 168.540005 \nL 197.720467 168.947341 \nL 198.330414 168.260552 \nL 200.160256 169.739998 \nL 200.770203 169.933726 \nL 201.38015 170.414998 \nL 202.600044 170.217881 \nL 203.819939 170.592629 \nL 204.429886 170.494582 \nL 205.039833 169.833912 \nL 205.64978 169.739998 \nL 206.259727 169.926204 \nL 206.869675 169.832705 \nL 210.529358 170.914896 \nL 211.139305 171.359997 \nL 211.749252 171.532529 \nL 213.579093 171.244917 \nL 214.189041 171.67959 \nL 214.798988 171.583909 \nL 216.018882 172.440005 \nL 217.848724 172.924068 \nL 219.068618 172.215891 \nL 219.678565 172.121108 \nL 220.288512 172.5353 \nL 222.118354 172.251627 \nL 222.728301 172.40873 \nL 223.338248 171.816926 \nL 224.558142 171.636186 \nL 225.16809 171.300458 \nL 225.778037 171.703643 \nL 226.387984 171.859248 \nL 226.997931 171.526465 \nL 228.217825 171.351944 \nL 228.827773 171.747432 \nL 229.43772 171.900006 \nL 230.047667 170.616759 \nL 231.877508 170.370658 \nL 232.487456 170.761096 \nL 233.097403 170.679136 \nL 233.70735 170.363831 \nL 234.317297 170.750069 \nL 234.927244 170.901291 \nL 235.537191 170.588573 \nL 236.147139 169.586266 \nL 236.757086 169.280423 \nL 237.367033 169.434696 \nL 237.97698 169.131548 \nL 238.586927 169.51263 \nL 239.196874 168.984758 \nL 239.806822 169.363696 \nL 240.416769 169.290002 \nL 241.026716 169.44104 \nL 242.24661 168.403924 \nL 242.856557 168.556439 \nL 243.466505 168.929082 \nL 244.076452 168.858371 \nL 244.686399 169.007797 \nL 245.296346 168.718379 \nL 247.126188 169.16208 \nL 248.346082 169.883524 \nL 248.956029 169.81152 \nL 249.565976 170.167728 \nL 250.175923 170.09526 \nL 250.785871 170.235735 \nL 252.615712 170.020523 \nL 253.225659 169.739998 \nL 255.055501 170.155388 \nL 256.885342 169.945718 \nL 257.495289 170.286834 \nL 259.325131 170.687962 \nL 259.935078 170.414998 \nL 260.545025 170.749345 \nL 261.154972 170.679136 \nL 261.76492 171.010587 \nL 262.374867 171.140005 \nL 262.984814 170.869851 \nL 263.594761 170.800125 \nL 264.204708 170.928994 \nL 264.814655 170.661949 \nL 265.424603 169.805659 \nL 266.03455 169.739998 \nL 266.644497 169.870518 \nL 267.254444 169.414699 \nL 268.474338 169.675332 \nL 269.084286 169.611043 \nL 270.30418 169.099056 \nL 270.914127 169.037045 \nL 272.134021 169.676472 \nL 272.743969 169.613312 \nL 273.353916 169.361051 \nL 273.963863 169.677032 \nL 274.57381 169.614423 \nL 275.793704 169.86486 \nL 277.013599 169.739998 \nL 277.623546 169.863788 \nL 278.233493 169.616576 \nL 278.84344 169.186152 \nL 280.063335 169.066911 \nL 280.673282 169.190845 \nL 281.283229 169.131548 \nL 281.893176 168.890564 \nL 282.503123 168.832435 \nL 283.11307 169.136646 \nL 284.332965 169.020002 \nL 286.772753 169.502636 \nL 288.602595 170.387411 \nL 289.212542 170.326955 \nL 289.822489 170.44244 \nL 290.432436 170.382167 \nL 291.042384 170.496871 \nL 291.652331 170.262579 \nL 292.872225 170.83733 \nL 294.092119 171.06128 \nL 294.702067 171.000477 \nL 295.312014 170.76857 \nL 295.921961 170.708867 \nL 296.531908 170.819997 \nL 297.141855 170.590398 \nL 297.751802 170.531622 \nL 299.581644 170.862078 \nL 300.191591 170.803215 \nL 301.411485 171.020416 \nL 302.021433 170.961592 \nL 302.63138 170.736927 \nL 303.241327 170.679136 \nL 303.851274 170.291025 \nL 305.071168 170.507511 \nL 306.291063 171.049094 \nL 306.90101 170.99139 \nL 308.120904 171.201659 \nL 308.730851 171.144003 \nL 309.340799 171.248229 \nL 309.950746 171.029551 \nL 310.560693 170.651173 \nL 311.17064 170.916238 \nL 311.780587 170.860002 \nL 312.390534 170.48483 \nL 313.610429 171.010587 \nL 314.220376 170.954669 \nL 315.44027 171.474304 \nL 316.050217 171.417672 \nL 316.660165 171.675109 \nL 317.270112 171.461742 \nL 318.490006 171.349617 \nL 319.099953 171.604746 \nL 320.319848 171.183438 \nL 320.929795 170.819997 \nL 321.539742 170.920052 \nL 322.149689 170.866066 \nL 322.759636 170.965532 \nL 323.369583 170.758866 \nL 323.979531 170.858119 \nL 325.199425 170.75171 \nL 326.419319 170.3442 \nL 327.029266 169.991169 \nL 327.639214 169.940466 \nL 328.249161 170.190004 \nL 328.859108 169.839773 \nL 330.688949 169.690463 \nL 331.298897 169.789427 \nL 331.908844 169.59206 \nL 332.518791 169.690801 \nL 333.128738 169.641817 \nL 333.738685 169.446119 \nL 334.95858 169.935039 \nL 336.178474 170.128313 \nL 336.788421 170.079019 \nL 338.008315 170.270361 \nL 338.618263 170.509713 \nL 340.448104 170.791329 \nL 341.058051 170.598277 \nL 341.667998 170.54881 \nL 342.277946 170.784396 \nL 342.887893 170.734735 \nL 344.107787 170.919038 \nL 345.327681 171.38348 \nL 345.937629 171.473628 \nL 346.547576 171.423118 \nL 347.157523 171.512783 \nL 347.76747 171.183109 \nL 348.377417 171.133545 \nL 348.987365 170.667037 \nL 349.597312 170.618806 \nL 350.207259 170.709234 \nL 350.817206 170.384775 \nL 352.0371 170.565476 \nL 352.647048 170.517969 \nL 353.256995 170.607653 \nL 353.866942 170.423543 \nL 355.086836 170.329919 \nL 355.696783 170.147547 \nL 356.306731 170.101508 \nL 356.916678 169.785092 \nL 357.526625 169.874998 \nL 358.136572 169.829818 \nL 359.966414 170.097027 \nL 360.576361 169.918151 \nL 361.186308 169.873337 \nL 361.796255 169.961767 \nL 363.016149 169.872517 \nL 363.626097 169.960405 \nL 364.236044 170.179924 \nL 364.845991 170.13512 \nL 365.455938 170.221946 \nL 366.065885 170.439601 \nL 366.675832 170.394546 \nL 367.28578 170.21903 \nL 367.895727 170.174613 \nL 368.505674 170.000246 \nL 369.115621 170.08629 \nL 369.725568 170.301598 \nL 369.725568 170.301598 \n\" clip-path=\"url(#p723db9eccb)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 65.361932 148.14 \nL 65.971879 212.94 \nL 66.581826 169.739998 \nL 67.191773 196.740001 \nL 67.80172 174.060004 \nL 68.411668 180.539998 \nL 69.021615 166.654284 \nL 69.631562 172.440005 \nL 70.241509 169.739998 \nL 71.461403 165.812729 \nL 72.071351 169.739998 \nL 72.681298 163.093845 \nL 73.291245 162.025713 \nL 73.901192 156.780001 \nL 74.511139 160.289998 \nL 75.121086 159.575293 \nL 75.731034 166.139999 \nL 76.950928 170.819997 \nL 77.560875 175.911427 \nL 78.170822 174.649094 \nL 78.780769 176.313916 \nL 79.390717 175.140003 \nL 80.000664 179.244005 \nL 80.610611 180.539998 \nL 81.220558 179.340004 \nL 81.830505 173.597146 \nL 83.0504 180.539998 \nL 84.270294 182.565005 \nL 86.100135 190.722856 \nL 87.32003 191.923786 \nL 87.929977 194.18211 \nL 88.539924 193.001535 \nL 89.149871 187.020001 \nL 89.759819 186.071709 \nL 90.979713 190.335353 \nL 91.58966 186.430911 \nL 92.199607 179.820002 \nL 92.809554 179.13131 \nL 93.419502 179.85064 \nL 94.029449 181.890006 \nL 94.639396 182.523678 \nL 95.249343 181.836 \nL 95.85929 183.716468 \nL 96.469237 180.539998 \nL 97.079185 179.928679 \nL 97.689132 178.140001 \nL 98.299079 178.772727 \nL 98.909026 177.068577 \nL 99.518973 176.56105 \nL 100.738868 177.794241 \nL 101.958762 176.821973 \nL 102.568709 177.404517 \nL 103.178656 175.911427 \nL 103.788603 177.502505 \nL 104.398551 176.053852 \nL 105.008498 175.630911 \nL 106.838339 177.253044 \nL 107.448286 176.837143 \nL 108.058234 177.345635 \nL 109.278128 180.096162 \nL 110.498022 180.972005 \nL 111.107969 179.687367 \nL 112.327864 178.878459 \nL 112.937811 177.669119 \nL 113.547758 178.919999 \nL 114.767652 179.749754 \nL 115.987547 182.082855 \nL 116.597494 181.683533 \nL 117.207441 182.046983 \nL 117.817388 180.167587 \nL 118.427335 179.803635 \nL 119.64723 180.539998 \nL 120.867124 179.835654 \nL 121.477071 178.798064 \nL 122.087018 177.09319 \nL 122.696966 177.470525 \nL 123.306913 176.490002 \nL 123.91686 176.865773 \nL 124.526807 175.250206 \nL 125.746701 174.708005 \nL 126.356649 175.728118 \nL 127.576543 176.450682 \nL 128.18649 176.178462 \nL 131.236226 177.864768 \nL 131.846173 177.594546 \nL 132.45612 176.745411 \nL 133.066067 176.490002 \nL 133.676015 177.386016 \nL 134.285962 177.697894 \nL 135.505856 177.188281 \nL 136.115803 177.493844 \nL 136.72575 176.146781 \nL 137.335698 176.455964 \nL 137.945645 176.220002 \nL 138.555592 176.523469 \nL 139.165539 177.353118 \nL 139.775486 176.588782 \nL 140.385433 177.404517 \nL 140.995381 177.688798 \nL 141.605328 177.454284 \nL 142.215275 177.733698 \nL 142.825222 177.502505 \nL 143.435169 178.279539 \nL 144.655064 178.808705 \nL 145.265011 179.55818 \nL 145.874958 179.321957 \nL 146.484905 179.572838 \nL 147.094852 179.340004 \nL 147.704799 179.587061 \nL 148.314747 178.884523 \nL 148.924694 178.661741 \nL 149.534641 178.908344 \nL 150.144588 178.688575 \nL 150.754535 178.93149 \nL 151.364482 178.714646 \nL 151.97443 179.407132 \nL 154.414218 178.556327 \nL 155.024165 179.226489 \nL 155.634113 179.452757 \nL 156.24406 178.811997 \nL 157.463954 179.261057 \nL 158.073901 178.634115 \nL 158.683848 178.436101 \nL 159.293796 178.658709 \nL 159.903743 177.216921 \nL 161.733584 177.890945 \nL 162.343531 177.705 \nL 162.953479 177.118884 \nL 163.563426 176.939998 \nL 164.173373 177.160858 \nL 164.78332 176.983903 \nL 165.393267 177.594546 \nL 166.003214 177.417108 \nL 166.613162 178.017844 \nL 167.223109 178.225717 \nL 167.833056 177.664262 \nL 169.05295 178.834737 \nL 169.662897 177.526047 \nL 170.272845 176.981625 \nL 170.882792 177.560692 \nL 171.492739 177.392572 \nL 172.102686 177.594546 \nL 172.712633 178.160336 \nL 173.932528 177.100895 \nL 175.152422 177.496905 \nL 175.762369 177.335602 \nL 176.372316 177.53017 \nL 176.982263 177.018259 \nL 177.592211 176.862161 \nL 178.202158 177.05613 \nL 178.812105 177.594546 \nL 180.031999 177.968569 \nL 180.641946 178.493689 \nL 181.861841 178.852504 \nL 184.301629 180.870617 \nL 186.131471 180.377189 \nL 186.741418 180.539998 \nL 187.351365 179.734035 \nL 187.961312 179.577627 \nL 188.57126 179.103549 \nL 189.181207 179.269409 \nL 189.791154 179.11756 \nL 190.401101 178.652626 \nL 191.620995 179.605388 \nL 192.230943 179.454833 \nL 192.84089 179.922857 \nL 194.670731 179.475216 \nL 195.280678 178.723182 \nL 195.890626 178.58093 \nL 196.500573 178.140001 \nL 197.11052 177.404517 \nL 197.720467 177.27028 \nL 198.330414 177.729044 \nL 200.160256 177.329191 \nL 200.770203 176.617132 \nL 201.990097 175.788004 \nL 202.600044 176.239121 \nL 203.209992 176.115331 \nL 203.819939 176.56105 \nL 205.039833 176.877389 \nL 205.64978 176.472466 \nL 207.479622 176.109229 \nL 208.089569 176.541699 \nL 208.699516 176.146781 \nL 209.91941 176.455964 \nL 210.529358 176.066366 \nL 211.139305 176.490002 \nL 211.749252 176.641243 \nL 212.359199 176.523469 \nL 212.969146 176.673339 \nL 213.579093 176.290819 \nL 214.189041 176.175922 \nL 215.408935 176.473606 \nL 216.018882 176.359356 \nL 217.848724 176.796578 \nL 219.068618 176.57004 \nL 219.678565 176.968348 \nL 220.288512 177.109412 \nL 220.898459 176.996253 \nL 221.508407 176.379692 \nL 222.118354 176.521393 \nL 222.728301 176.411817 \nL 223.948195 177.188281 \nL 224.558142 177.077402 \nL 225.16809 177.460154 \nL 225.778037 177.349092 \nL 226.387984 177.483396 \nL 227.607878 177.263598 \nL 228.217825 177.638509 \nL 229.43772 176.939998 \nL 230.047667 177.311955 \nL 231.877508 177.702046 \nL 232.487456 177.358912 \nL 233.097403 177.722613 \nL 233.70735 177.849753 \nL 234.317297 177.509786 \nL 234.927244 177.636771 \nL 235.537191 177.531435 \nL 236.147139 177.888048 \nL 236.757086 177.552765 \nL 237.367033 177.90679 \nL 237.97698 178.030145 \nL 238.586927 177.697894 \nL 239.196874 177.594546 \nL 239.806822 177.943483 \nL 240.416769 177.614997 \nL 241.026716 177.737232 \nL 242.856557 178.764654 \nL 243.466505 178.881298 \nL 244.076452 178.556327 \nL 244.686399 178.453221 \nL 245.296346 178.788649 \nL 245.906293 178.903642 \nL 246.51624 178.365506 \nL 247.126188 178.481136 \nL 248.346082 177.848971 \nL 249.565976 178.080598 \nL 250.175923 177.982107 \nL 250.785871 178.096723 \nL 251.395818 177.787057 \nL 252.005765 177.901567 \nL 253.225659 177.70893 \nL 253.835606 178.031613 \nL 255.055501 177.84 \nL 255.665448 178.159168 \nL 256.275395 178.269941 \nL 256.885342 177.968569 \nL 257.495289 178.079246 \nL 258.105237 177.984801 \nL 258.715184 177.483396 \nL 259.325131 177.594546 \nL 259.935078 177.502505 \nL 261.76492 177.831638 \nL 262.374867 177.539999 \nL 263.594761 177.359636 \nL 264.204708 176.873951 \nL 264.814655 176.588782 \nL 265.424603 176.896237 \nL 266.03455 176.809092 \nL 266.644497 177.114017 \nL 267.254444 177.221932 \nL 267.864391 177.523787 \nL 269.084286 177.348358 \nL 269.694233 177.068577 \nL 270.914127 177.664262 \nL 271.524074 177.577165 \nL 272.134021 177.681179 \nL 272.743969 177.404517 \nL 273.353916 177.508424 \nL 273.963863 177.800643 \nL 274.57381 177.902793 \nL 275.183757 177.816527 \nL 275.793704 177.543466 \nL 276.403652 177.645481 \nL 277.013599 177.560692 \nL 278.84344 177.863078 \nL 280.063335 177.694678 \nL 280.673282 177.794241 \nL 282.503123 177.54504 \nL 283.11307 177.824918 \nL 283.723018 177.922732 \nL 284.332965 177.84 \nL 284.942912 178.11673 \nL 286.162806 177.594546 \nL 286.772753 177.157585 \nL 287.992648 176.999015 \nL 288.602595 176.743818 \nL 289.212542 177.018259 \nL 291.652331 177.404517 \nL 292.872225 177.248023 \nL 294.702067 177.532043 \nL 295.312014 177.454284 \nL 295.921961 177.718895 \nL 297.751802 177.486602 \nL 298.36175 177.748356 \nL 300.191591 177.518245 \nL 300.801538 177.274886 \nL 301.411485 177.53382 \nL 302.63138 177.71539 \nL 303.241327 177.639745 \nL 303.851274 177.399187 \nL 304.461221 177.32473 \nL 305.071168 177.579598 \nL 305.681116 177.669119 \nL 306.291063 176.939998 \nL 306.90101 177.030677 \nL 307.510957 176.958093 \nL 308.120904 176.723463 \nL 309.340799 176.904087 \nL 311.17064 177.652878 \nL 312.390534 177.188281 \nL 313.000482 177.276122 \nL 313.610429 177.045885 \nL 314.220376 176.975213 \nL 314.830323 177.062928 \nL 315.44027 176.67723 \nL 317.880059 176.402171 \nL 321.539742 176.922897 \nL 322.149689 177.161805 \nL 323.369583 177.330571 \nL 324.589478 177.801975 \nL 325.809372 177.360563 \nL 327.029266 177.526047 \nL 327.639214 177.307523 \nL 328.249161 177.239999 \nL 328.859108 177.322451 \nL 329.469055 177.553827 \nL 330.079002 177.486206 \nL 332.518791 177.809246 \nL 333.128738 177.741818 \nL 333.738685 177.968569 \nL 334.348632 177.901084 \nL 334.95858 177.687629 \nL 335.568527 177.91297 \nL 336.178474 177.700452 \nL 336.788421 177.634174 \nL 337.398368 177.858124 \nL 338.008315 177.936434 \nL 338.618263 177.870069 \nL 339.22821 177.948002 \nL 339.838157 177.881907 \nL 340.448104 177.529378 \nL 342.277946 177.762859 \nL 342.887893 177.41369 \nL 343.49784 177.207835 \nL 344.717734 176.516468 \nL 345.327681 176.595657 \nL 345.937629 176.533926 \nL 346.547576 176.612729 \nL 347.76747 176.490002 \nL 348.377417 176.568389 \nL 349.597312 177.00167 \nL 350.817206 177.15493 \nL 352.0371 177.03172 \nL 353.256995 177.18355 \nL 353.866942 177.395701 \nL 354.476889 177.061267 \nL 355.696783 177.211697 \nL 356.916678 177.090312 \nL 357.526625 177.165001 \nL 358.136572 177.374101 \nL 358.746519 177.447882 \nL 359.356466 177.253044 \nL 360.576361 177.40021 \nL 361.186308 177.340005 \nL 362.406202 176.290819 \nL 363.016149 176.498286 \nL 363.626097 176.440409 \nL 364.236044 176.51475 \nL 364.845991 176.720489 \nL 365.455938 176.793961 \nL 366.065885 176.735948 \nL 366.675832 176.809092 \nL 367.28578 176.751292 \nL 368.505674 177.156871 \nL 369.725568 177.300001 \nL 369.725568 177.300001 \n\" clip-path=\"url(#p723db9eccb)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 65.361932 212.94 \nL 65.971879 148.14 \nL 66.581826 148.14 \nL 67.191773 115.740002 \nL 67.80172 122.220006 \nL 68.411668 126.540002 \nL 69.021615 120.368573 \nL 69.631562 123.840004 \nL 70.241509 140.940001 \nL 70.851456 135.180003 \nL 71.461403 142.249097 \nL 72.071351 142.740005 \nL 72.681298 153.124615 \nL 73.901192 143.820004 \nL 75.121086 159.575293 \nL 76.340981 158.371582 \nL 77.560875 163.56857 \nL 78.170822 156.976365 \nL 78.780769 153.774789 \nL 79.390717 158.939999 \nL 80.000664 161.099997 \nL 80.610611 158.10923 \nL 81.220558 162.539999 \nL 81.830505 164.340004 \nL 82.440452 161.546894 \nL 83.0504 163.260005 \nL 83.660347 160.681933 \nL 84.270294 164.340004 \nL 84.880241 165.812729 \nL 86.100135 157.397142 \nL 86.710083 157.140004 \nL 87.929977 153.255791 \nL 88.539924 149.801538 \nL 89.149871 151.380007 \nL 89.759819 149.720486 \nL 90.369766 152.768571 \nL 90.979713 152.660936 \nL 91.58966 155.503639 \nL 92.199607 156.780001 \nL 92.809554 155.183476 \nL 93.419502 157.791066 \nL 94.029449 158.939999 \nL 94.639396 161.364493 \nL 95.249343 162.396 \nL 97.689132 161.340005 \nL 98.299079 159.921816 \nL 98.909026 162.025713 \nL 100.738868 158.02475 \nL 101.348815 157.86 \nL 102.568709 159.636773 \nL 103.178656 158.425714 \nL 103.788603 158.265 \nL 104.398551 159.106159 \nL 105.008498 160.903634 \nL 105.618445 160.713131 \nL 106.228392 162.434121 \nL 106.838339 161.287825 \nL 107.448286 161.099997 \nL 108.058234 161.830142 \nL 108.668181 163.440001 \nL 109.888075 163.026485 \nL 110.498022 164.555998 \nL 112.327864 166.416922 \nL 113.547758 169.199999 \nL 114.157705 167.340002 \nL 114.767652 167.896098 \nL 115.3776 169.219523 \nL 115.987547 169.739998 \nL 116.597494 168.723536 \nL 117.207441 168.484184 \nL 117.817388 168.995177 \nL 118.427335 168.758181 \nL 119.037283 169.254613 \nL 120.867124 168.566086 \nL 122.087018 169.510216 \nL 122.696966 169.285261 \nL 124.526807 172.605306 \nL 125.136754 172.358181 \nL 125.746701 172.764001 \nL 126.356649 171.237028 \nL 128.18649 170.570768 \nL 129.406384 168.92491 \nL 130.016332 169.941875 \nL 130.626279 169.139997 \nL 131.236226 168.947341 \nL 131.846173 168.169091 \nL 132.45612 167.98865 \nL 133.066067 168.39 \nL 133.676015 167.063899 \nL 135.505856 166.574488 \nL 136.115803 166.970768 \nL 136.72575 166.26204 \nL 137.335698 166.109747 \nL 137.945645 166.500002 \nL 138.555592 166.348268 \nL 139.775486 168.159512 \nL 140.385433 168.520645 \nL 140.995381 168.357604 \nL 142.825222 169.402504 \nL 144.655064 168.915573 \nL 145.265011 168.267273 \nL 145.874958 169.090375 \nL 147.094852 169.739998 \nL 147.704799 169.581177 \nL 148.314747 168.951677 \nL 148.924694 169.270439 \nL 149.534641 168.652236 \nL 150.754535 169.280423 \nL 151.364482 169.131548 \nL 151.97443 169.437902 \nL 152.584377 169.290002 \nL 155.634113 170.754771 \nL 156.24406 170.604003 \nL 156.854007 171.313513 \nL 157.463954 171.587366 \nL 158.073901 171.010587 \nL 158.683848 170.020523 \nL 159.293796 170.297417 \nL 159.903743 170.986157 \nL 160.51369 171.253376 \nL 161.123637 169.876707 \nL 161.733584 170.555096 \nL 162.953479 170.276647 \nL 163.563426 170.540004 \nL 164.173373 169.60749 \nL 164.78332 169.871706 \nL 165.393267 169.347271 \nL 166.003214 170.000246 \nL 167.223109 169.739998 \nL 167.833056 170.379058 \nL 168.443003 169.867061 \nL 169.05295 169.739998 \nL 169.662897 170.367906 \nL 170.882792 169.367588 \nL 172.102686 169.126363 \nL 174.542475 170.100001 \nL 175.152422 169.978674 \nL 175.762369 170.570768 \nL 176.372316 169.739998 \nL 177.592211 170.207028 \nL 178.202158 170.088385 \nL 178.812105 170.664063 \nL 180.031999 170.425716 \nL 180.641946 169.967367 \nL 181.251894 170.19236 \nL 181.861841 169.739998 \nL 182.471788 168.620835 \nL 183.081735 168.849275 \nL 183.691682 169.407698 \nL 184.301629 168.637965 \nL 184.911577 168.204972 \nL 185.521524 168.103643 \nL 186.741418 168.551998 \nL 187.351365 168.450446 \nL 187.961312 168.670698 \nL 188.57126 168.250346 \nL 189.181207 168.46941 \nL 189.791154 168.370244 \nL 190.401101 168.586605 \nL 191.011048 168.174788 \nL 191.620995 168.07846 \nL 192.230943 168.603155 \nL 193.450837 168.409196 \nL 194.060784 168.619251 \nL 194.670731 169.131548 \nL 195.890626 169.539068 \nL 196.500573 170.039999 \nL 197.11052 170.237695 \nL 197.720467 170.136337 \nL 198.330414 170.331783 \nL 198.940361 169.347271 \nL 200.770203 169.933726 \nL 201.990097 169.739998 \nL 203.209992 168.978771 \nL 204.429886 168.796766 \nL 205.039833 169.270439 \nL 206.869675 169.832705 \nL 208.699516 169.556951 \nL 209.91941 168.832435 \nL 210.529358 169.288119 \nL 211.749252 168.574853 \nL 212.969146 168.406666 \nL 213.579093 168.589183 \nL 214.189041 168.505717 \nL 214.798988 168.686341 \nL 217.238776 167.320806 \nL 217.848724 166.728046 \nL 219.678565 166.508508 \nL 220.288512 166.690591 \nL 220.898459 166.618123 \nL 221.508407 167.050507 \nL 222.118354 167.228369 \nL 222.728301 166.90448 \nL 223.338248 167.330771 \nL 223.948195 167.257241 \nL 224.558142 167.678935 \nL 225.16809 167.358252 \nL 225.778037 167.285455 \nL 226.997931 167.628724 \nL 227.607878 168.04112 \nL 228.827773 167.411379 \nL 230.047667 167.747386 \nL 231.267561 167.603735 \nL 231.877508 167.769199 \nL 233.70735 167.556604 \nL 234.927244 167.881932 \nL 235.537191 168.274283 \nL 236.147139 168.202636 \nL 236.757086 168.591066 \nL 237.367033 168.289829 \nL 237.97698 168.447047 \nL 238.586927 168.375796 \nL 239.196874 168.531614 \nL 239.806822 168.460556 \nL 240.416769 168.615003 \nL 242.856557 168.334526 \nL 243.466505 168.486762 \nL 244.076452 168.858371 \nL 244.686399 168.348817 \nL 245.296346 168.49946 \nL 245.906293 168.212726 \nL 246.51624 168.58027 \nL 247.126188 168.511906 \nL 247.736135 168.659999 \nL 248.346082 168.591828 \nL 248.956029 168.094972 \nL 250.175923 168.39 \nL 250.785871 168.111145 \nL 251.395818 168.46941 \nL 252.005765 168.19212 \nL 253.225659 168.901167 \nL 254.445554 168.767654 \nL 255.055501 168.28616 \nL 255.665448 168.221793 \nL 256.275395 167.951465 \nL 256.885342 167.888576 \nL 257.495289 168.2362 \nL 258.715184 168.109813 \nL 259.325131 167.844081 \nL 259.935078 167.782505 \nL 261.154972 166.855534 \nL 261.76492 166.99821 \nL 262.984814 166.882155 \nL 263.594761 167.023441 \nL 264.814655 167.698537 \nL 265.424603 167.836047 \nL 266.644497 167.717038 \nL 267.254444 168.04844 \nL 268.474338 167.929227 \nL 269.694233 168.197141 \nL 270.914127 168.845326 \nL 271.524074 168.975401 \nL 272.134021 168.532946 \nL 272.743969 168.473137 \nL 273.353916 168.603155 \nL 274.57381 168.107447 \nL 275.183757 168.425215 \nL 275.793704 168.366594 \nL 276.403652 168.495047 \nL 277.013599 168.250346 \nL 277.623546 168.192719 \nL 278.233493 168.320573 \nL 278.84344 168.263082 \nL 279.453387 168.57409 \nL 280.673282 168.458644 \nL 282.503123 168.832435 \nL 283.11307 168.593634 \nL 283.723018 168.536654 \nL 284.332965 168.659999 \nL 284.942912 168.603155 \nL 285.552859 168.367627 \nL 286.162806 167.954874 \nL 287.382701 168.201371 \nL 288.602595 168.092046 \nL 289.822489 167.632683 \nL 290.432436 167.75514 \nL 291.652331 167.649678 \nL 292.262278 167.423651 \nL 292.872225 167.372089 \nL 294.092119 167.614472 \nL 294.702067 167.562813 \nL 295.312014 167.854287 \nL 295.921961 167.973249 \nL 296.531908 168.262107 \nL 297.751802 168.496022 \nL 298.971697 168.39 \nL 300.191591 168.620835 \nL 300.801538 168.567911 \nL 302.021433 168.796042 \nL 303.241327 168.690387 \nL 303.851274 168.803265 \nL 305.071168 168.698382 \nL 305.681116 168.482282 \nL 306.291063 168.758181 \nL 307.510957 168.328945 \nL 308.120904 168.278348 \nL 309.340799 168.824295 \nL 309.950746 168.772839 \nL 310.560693 168.882434 \nL 311.780587 168.460005 \nL 312.390534 168.729165 \nL 313.000482 168.837794 \nL 313.610429 168.787062 \nL 314.220376 168.895016 \nL 314.830323 168.84439 \nL 316.050217 168.429319 \nL 317.270112 168.644347 \nL 317.880059 168.594938 \nL 318.490006 168.701539 \nL 319.099953 168.652236 \nL 319.7099 168.913207 \nL 320.319848 169.018284 \nL 320.929795 168.968575 \nL 322.759636 168.361283 \nL 323.369583 168.619251 \nL 325.809372 169.033463 \nL 326.419319 168.531614 \nL 327.029266 168.634884 \nL 327.639214 168.888025 \nL 328.859108 168.792199 \nL 330.079002 168.101383 \nL 330.688949 168.204219 \nL 331.298897 168.158305 \nL 332.518791 168.362326 \nL 333.128738 168.610909 \nL 333.738685 168.564492 \nL 334.348632 168.664885 \nL 334.95858 168.472278 \nL 336.178474 168.380904 \nL 336.788421 168.480804 \nL 337.398368 168.435305 \nL 338.008315 168.534646 \nL 338.618263 168.056261 \nL 339.22821 168.011998 \nL 339.838157 167.680577 \nL 341.058051 168.166493 \nL 342.277946 168.07846 \nL 342.887893 168.318951 \nL 343.49784 168.416592 \nL 344.107787 168.37232 \nL 345.327681 167.720871 \nL 345.937629 167.818956 \nL 346.547576 167.776364 \nL 347.76747 168.250346 \nL 348.987365 168.442151 \nL 349.597312 168.398671 \nL 350.207259 168.493849 \nL 350.817206 168.72678 \nL 351.427153 168.820848 \nL 352.0371 168.364209 \nL 352.647048 168.595932 \nL 353.256995 168.278696 \nL 353.866942 168.2362 \nL 354.476889 168.466736 \nL 355.086836 168.560167 \nL 355.696783 168.789061 \nL 357.526625 168.659999 \nL 358.136572 168.21318 \nL 359.356466 167.861742 \nL 359.966414 167.820994 \nL 360.576361 167.914019 \nL 361.186308 167.74 \nL 361.796255 167.699754 \nL 363.016149 168.149818 \nL 363.626097 168.108983 \nL 364.236044 168.200289 \nL 364.845991 168.159512 \nL 365.455938 168.381782 \nL 366.065885 168.340812 \nL 366.675832 168.561818 \nL 367.895727 168.479636 \nL 368.505674 168.308677 \nL 369.725568 168.228002 \nL 369.725568 168.228002 \n\" clip-path=\"url(#p723db9eccb)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 65.361932 212.94 \nL 65.971879 245.34 \nL 66.581826 191.339997 \nL 67.191773 180.539998 \nL 67.80172 187.020001 \nL 68.411668 191.339997 \nL 69.021615 194.425711 \nL 69.631562 180.539998 \nL 70.241509 184.139997 \nL 71.461403 177.594546 \nL 72.681298 173.063075 \nL 73.901192 187.020001 \nL 74.511139 176.490002 \nL 75.121086 178.634115 \nL 75.731034 169.739998 \nL 76.950928 174.060004 \nL 77.560875 172.825712 \nL 78.170822 174.649094 \nL 78.780769 173.496521 \nL 79.390717 167.040001 \nL 80.000664 163.692002 \nL 80.610611 165.586153 \nL 81.830505 173.597146 \nL 82.440452 177.188281 \nL 83.0504 178.38 \nL 83.660347 181.585168 \nL 84.270294 180.539998 \nL 84.880241 177.594546 \nL 85.490188 178.634115 \nL 86.100135 177.762859 \nL 87.929977 180.539998 \nL 89.149871 185.400003 \nL 90.979713 182.800466 \nL 91.58966 182.012733 \nL 92.809554 186.174786 \nL 93.419502 183.986806 \nL 94.029449 184.590003 \nL 94.639396 181.201228 \nL 96.469237 183.032305 \nL 97.079185 181.151326 \nL 97.689132 182.940004 \nL 98.299079 179.950907 \nL 98.909026 179.382857 \nL 99.518973 181.108424 \nL 101.348815 179.459999 \nL 103.178656 181.054283 \nL 103.788603 179.527503 \nL 104.398551 181.038467 \nL 105.008498 181.521815 \nL 105.618445 182.957916 \nL 106.228392 181.492944 \nL 106.838339 179.13131 \nL 107.448286 179.614291 \nL 108.058234 180.996338 \nL 108.668181 180.539998 \nL 109.888075 177.91297 \nL 110.498022 178.38 \nL 111.107969 177.129477 \nL 111.717917 176.752991 \nL 112.327864 177.216921 \nL 113.547758 174.870003 \nL 115.987547 173.597146 \nL 116.597494 174.822351 \nL 117.207441 175.265578 \nL 117.817388 174.208966 \nL 118.427335 174.649094 \nL 119.64723 174.060004 \nL 120.257177 175.199339 \nL 120.867124 174.905218 \nL 122.087018 175.714474 \nL 123.306913 175.140003 \nL 123.91686 172.857529 \nL 124.526807 172.605306 \nL 125.136754 171.703643 \nL 125.746701 172.764001 \nL 126.356649 171.878618 \nL 126.966596 172.916469 \nL 127.576543 172.046796 \nL 129.406384 173.204149 \nL 130.016332 172.969905 \nL 130.626279 173.94 \nL 132.45612 174.994053 \nL 133.066067 174.175711 \nL 134.285962 174.85579 \nL 134.895909 175.750433 \nL 135.505856 176.071039 \nL 136.115803 175.832306 \nL 136.72575 176.695934 \nL 137.335698 177.000502 \nL 137.945645 176.760001 \nL 138.555592 177.059007 \nL 139.775486 176.588782 \nL 140.385433 175.836776 \nL 140.995381 175.615201 \nL 141.605328 174.882855 \nL 142.215275 174.672288 \nL 143.435169 172.251627 \nL 144.655064 172.872824 \nL 145.265011 171.212724 \nL 145.874958 170.552035 \nL 146.484905 170.384775 \nL 147.704799 171.010587 \nL 148.314747 169.89767 \nL 149.534641 168.652236 \nL 150.144588 168.505717 \nL 150.754535 167.901708 \nL 151.364482 167.762537 \nL 151.97443 168.531614 \nL 152.584377 167.940003 \nL 153.804271 168.556439 \nL 154.414218 169.299185 \nL 155.024165 169.156219 \nL 155.634113 169.450069 \nL 156.854007 169.167816 \nL 158.073901 169.739998 \nL 158.683848 169.178959 \nL 159.903743 168.909229 \nL 161.123637 169.46658 \nL 161.733584 169.332459 \nL 162.343531 168.794999 \nL 164.173373 169.60749 \nL 164.78332 169.081462 \nL 165.393267 168.169091 \nL 167.833056 169.228754 \nL 168.443003 169.104704 \nL 169.05295 167.845269 \nL 169.662897 168.484184 \nL 172.102686 169.494544 \nL 172.712633 170.106104 \nL 173.32258 170.34674 \nL 173.932528 170.22268 \nL 174.542475 169.739998 \nL 175.152422 169.978674 \nL 176.372316 169.739998 \nL 176.982263 169.974783 \nL 177.592211 169.156219 \nL 178.202158 169.391612 \nL 180.641946 168.944213 \nL 181.861841 167.377506 \nL 182.471788 167.613574 \nL 183.081735 166.845154 \nL 184.301629 165.993064 \nL 186.131471 166.700807 \nL 187.351365 167.80567 \nL 187.961312 167.387528 \nL 188.57126 167.611923 \nL 189.181207 166.88118 \nL 189.791154 167.105855 \nL 190.401101 167.013785 \nL 192.230943 167.673016 \nL 193.450837 167.487873 \nL 194.060784 167.702264 \nL 194.670731 167.306196 \nL 195.280678 167.822249 \nL 197.11052 167.550144 \nL 197.720467 167.163858 \nL 198.330414 167.076983 \nL 198.940361 167.285455 \nL 199.550309 166.905609 \nL 200.160256 166.821081 \nL 200.770203 167.027892 \nL 201.38015 167.522142 \nL 201.990097 167.148003 \nL 202.600044 167.350623 \nL 204.429886 167.098951 \nL 205.039833 166.734786 \nL 205.64978 166.934809 \nL 206.259727 167.412412 \nL 206.869675 167.60781 \nL 207.479622 167.524614 \nL 208.089569 166.614898 \nL 208.699516 166.536607 \nL 209.309463 166.185575 \nL 211.749252 165.886057 \nL 212.359199 166.080499 \nL 214.189041 165.860816 \nL 214.798988 166.315611 \nL 215.408935 166.504376 \nL 216.628829 165.836387 \nL 218.458671 166.397146 \nL 219.068618 166.837236 \nL 219.678565 166.763628 \nL 220.288512 166.436475 \nL 220.898459 166.618123 \nL 221.508407 167.050507 \nL 222.118354 166.726047 \nL 223.338248 167.081541 \nL 223.948195 166.760693 \nL 224.558142 166.689615 \nL 227.607878 167.555735 \nL 228.827773 167.411379 \nL 230.657614 167.913527 \nL 231.267561 167.841097 \nL 231.877508 167.532706 \nL 233.097403 167.861742 \nL 235.537191 167.58 \nL 236.147139 167.741429 \nL 236.757086 167.442133 \nL 238.586927 167.238952 \nL 239.196874 167.625316 \nL 239.806822 167.557425 \nL 240.416769 167.940003 \nL 241.026716 168.095715 \nL 241.636663 168.026897 \nL 242.24661 168.181238 \nL 242.856557 168.112604 \nL 243.466505 167.823282 \nL 244.076452 167.315514 \nL 244.686399 167.689837 \nL 245.296346 167.8427 \nL 245.906293 167.776364 \nL 246.51624 168.145366 \nL 247.126188 167.861742 \nL 247.736135 167.796004 \nL 248.346082 167.94598 \nL 248.956029 167.880397 \nL 249.565976 167.387528 \nL 250.175923 167.11105 \nL 250.785871 167.473775 \nL 251.395818 167.622351 \nL 252.005765 167.558893 \nL 253.225659 167.852627 \nL 253.835606 167.58 \nL 254.445554 167.517498 \nL 255.055501 167.66308 \nL 255.665448 167.600703 \nL 256.275395 167.951465 \nL 256.885342 167.888576 \nL 257.495289 167.621019 \nL 258.105237 167.763975 \nL 258.715184 167.498495 \nL 259.325131 167.031542 \nL 259.935078 167.175001 \nL 260.545025 167.519438 \nL 261.154972 167.660502 \nL 261.76492 167.399444 \nL 262.374867 167.540006 \nL 263.594761 167.420986 \nL 264.204708 167.163858 \nL 265.424603 167.836047 \nL 266.644497 168.108577 \nL 267.254444 168.04844 \nL 267.864391 167.599457 \nL 268.474338 167.541203 \nL 269.084286 167.289849 \nL 270.30418 167.560775 \nL 270.914127 167.503313 \nL 271.524074 166.872741 \nL 272.134021 166.817653 \nL 272.743969 166.952904 \nL 273.353916 166.897894 \nL 274.57381 167.165586 \nL 276.403652 167.001097 \nL 278.233493 167.394857 \nL 278.84344 167.15539 \nL 279.453387 166.733183 \nL 280.673282 166.994241 \nL 281.283229 166.941124 \nL 281.893176 167.07034 \nL 282.503123 167.01731 \nL 283.11307 166.602567 \nL 283.723018 166.551139 \nL 284.942912 166.808142 \nL 285.552859 166.756579 \nL 287.992648 167.261315 \nL 288.602595 167.209212 \nL 289.212542 167.333484 \nL 289.822489 167.105855 \nL 290.432436 167.40487 \nL 291.652331 167.301291 \nL 292.262278 166.902471 \nL 292.872225 167.198822 \nL 294.702067 167.047166 \nL 295.312014 167.16857 \nL 296.531908 166.72737 \nL 297.141855 166.848659 \nL 298.36175 166.750969 \nL 300.801538 167.228369 \nL 302.021433 166.797067 \nL 303.241327 166.701637 \nL 303.851274 166.984904 \nL 304.461221 167.101838 \nL 305.071168 166.560303 \nL 305.681116 166.513674 \nL 306.291063 166.630907 \nL 308.120904 166.491881 \nL 308.730851 166.608003 \nL 309.340799 166.400352 \nL 309.950746 166.516117 \nL 311.17064 166.425149 \nL 311.780587 166.540006 \nL 312.390534 166.494681 \nL 313.000482 166.768012 \nL 313.610429 166.88118 \nL 314.220376 167.152222 \nL 314.830323 166.789757 \nL 315.44027 166.744384 \nL 316.660165 166.968093 \nL 318.490006 166.832311 \nL 319.099953 166.942881 \nL 319.7099 166.742868 \nL 320.319848 166.69847 \nL 320.929795 166.808577 \nL 321.539742 166.610311 \nL 322.149689 166.720099 \nL 323.369583 166.632452 \nL 323.979531 166.74141 \nL 324.589478 166.545636 \nL 325.809372 166.762431 \nL 326.419319 167.021124 \nL 327.029266 166.977208 \nL 327.639214 166.482457 \nL 328.249161 166.139999 \nL 328.859108 166.248087 \nL 330.688949 166.123487 \nL 331.298897 165.78577 \nL 331.908844 165.893424 \nL 333.128738 165.518184 \nL 333.738685 165.772657 \nL 334.348632 165.439546 \nL 336.178474 165.759776 \nL 336.788421 166.01085 \nL 337.398368 165.82591 \nL 338.008315 165.497144 \nL 338.618263 165.747127 \nL 340.448104 166.060356 \nL 341.058051 165.7347 \nL 341.667998 165.69595 \nL 342.277946 165.799781 \nL 342.887893 165.618954 \nL 343.49784 165.864292 \nL 344.107787 165.96708 \nL 345.327681 166.453045 \nL 345.937629 166.553882 \nL 347.157523 166.194429 \nL 347.76747 166.155516 \nL 348.377417 165.838067 \nL 348.987365 165.80009 \nL 349.597312 165.901033 \nL 350.817206 165.825292 \nL 351.427153 165.92553 \nL 352.0371 166.162932 \nL 353.866942 166.458993 \nL 354.476889 166.420427 \nL 355.086836 166.109747 \nL 355.696783 166.072079 \nL 356.306731 166.305695 \nL 357.526625 166.500002 \nL 358.136572 166.731271 \nL 358.746519 166.692696 \nL 359.356466 166.922614 \nL 360.576361 166.577944 \nL 361.186308 166.540006 \nL 362.406202 166.995738 \nL 363.626097 166.654284 \nL 364.236044 166.352632 \nL 364.845991 166.315611 \nL 365.455938 165.884425 \nL 366.065885 165.979681 \nL 366.675832 165.812729 \nL 367.28578 165.777099 \nL 367.895727 165.871998 \nL 368.505674 165.836387 \nL 369.115621 165.671067 \nL 369.725568 165.765599 \nL 369.725568 165.765599 \n\" clip-path=\"url(#p723db9eccb)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 65.361932 212.94 \nL 65.971879 115.740002 \nL 66.581826 148.14 \nL 67.191773 164.340004 \nL 67.80172 161.099997 \nL 68.411668 137.340001 \nL 69.021615 148.14 \nL 69.631562 148.14 \nL 70.241509 133.740001 \nL 70.851456 148.14 \nL 71.461403 142.249097 \nL 72.071351 131.940006 \nL 72.681298 133.186155 \nL 73.291245 129.625716 \nL 73.901192 135.180003 \nL 74.511139 144.090004 \nL 75.121086 148.14 \nL 75.731034 155.339999 \nL 76.340981 158.371582 \nL 76.950928 157.86 \nL 77.560875 163.56857 \nL 78.780769 167.861742 \nL 79.390717 167.040001 \nL 80.610611 170.570768 \nL 81.220558 164.940005 \nL 82.440452 159.31242 \nL 83.0504 150.299998 \nL 83.660347 152.320651 \nL 84.270294 152.190006 \nL 84.880241 150.103634 \nL 85.490188 150.045882 \nL 86.100135 153.694287 \nL 86.710083 149.940005 \nL 87.32003 151.642706 \nL 87.929977 151.550531 \nL 89.149871 148.14 \nL 89.759819 149.720486 \nL 90.369766 152.768571 \nL 90.979713 154.167912 \nL 91.58966 156.976365 \nL 92.199607 156.780001 \nL 92.809554 159.409568 \nL 93.419502 159.169792 \nL 94.029449 156.240002 \nL 94.639396 156.074692 \nL 95.249343 157.211999 \nL 95.85929 157.034117 \nL 96.469237 159.355389 \nL 97.079185 160.366415 \nL 98.299079 159.921816 \nL 99.518973 154.961052 \nL 100.12892 155.960694 \nL 100.738868 154.72983 \nL 101.348815 155.700002 \nL 101.958762 153.451479 \nL 102.568709 155.456132 \nL 103.178656 156.368571 \nL 104.398551 154.121544 \nL 105.008498 154.030913 \nL 105.618445 154.910155 \nL 106.228392 153.857647 \nL 107.448286 155.54572 \nL 108.058234 155.441406 \nL 108.668181 153.540004 \nL 109.278128 154.353702 \nL 109.888075 153.394055 \nL 110.498022 153.324001 \nL 111.107969 154.108421 \nL 111.717917 154.030913 \nL 112.327864 153.124615 \nL 112.937811 153.881777 \nL 113.547758 153.000005 \nL 114.157705 153.739999 \nL 114.767652 152.881468 \nL 115.3776 152.824344 \nL 115.987547 150.45429 \nL 116.597494 151.189417 \nL 117.207441 148.893493 \nL 117.817388 150.374484 \nL 118.427335 150.349089 \nL 119.64723 148.860006 \nL 122.087018 151.586808 \nL 122.696966 150.868425 \nL 123.306913 151.515006 \nL 123.91686 151.480206 \nL 125.136754 154.030913 \nL 125.746701 153.324001 \nL 126.356649 153.914259 \nL 126.966596 153.857647 \nL 127.576543 154.431268 \nL 128.18649 155.616923 \nL 128.796437 156.16286 \nL 129.406384 155.475849 \nL 130.016332 154.196078 \nL 131.846173 154.030913 \nL 133.066067 155.082861 \nL 134.285962 153.824208 \nL 134.895909 153.774789 \nL 136.115803 155.893846 \nL 136.72575 156.37729 \nL 137.335698 156.308067 \nL 137.945645 156.780001 \nL 138.555592 157.779672 \nL 139.165539 156.638358 \nL 139.775486 156.042441 \nL 140.385433 156.501292 \nL 140.995381 155.916006 \nL 142.215275 156.814019 \nL 142.825222 156.746254 \nL 145.265011 158.449091 \nL 145.874958 157.884362 \nL 146.484905 158.295223 \nL 147.704799 158.145884 \nL 148.314747 159.01883 \nL 148.924694 158.939999 \nL 151.364482 160.46113 \nL 151.97443 159.468672 \nL 152.584377 159.840002 \nL 153.194324 159.31242 \nL 154.414218 159.160406 \nL 155.024165 158.21027 \nL 155.634113 157.707784 \nL 158.073901 159.151764 \nL 158.683848 159.921816 \nL 159.293796 159.009677 \nL 159.903743 159.770768 \nL 160.51369 159.283954 \nL 161.123637 159.623544 \nL 161.733584 158.73623 \nL 162.343531 158.669999 \nL 162.953479 159.007079 \nL 164.173373 158.87374 \nL 164.78332 159.203414 \nL 165.393267 159.136363 \nL 166.003214 157.899039 \nL 167.223109 157.782859 \nL 167.833056 157.342364 \nL 169.662897 158.312092 \nL 170.272845 159.002425 \nL 170.882792 159.31242 \nL 171.492739 158.137715 \nL 172.102686 157.712728 \nL 172.712633 158.390846 \nL 173.32258 158.333258 \nL 173.932528 158.638328 \nL 174.542475 159.300002 \nL 175.152422 159.238349 \nL 176.372316 159.825247 \nL 176.982263 159.409568 \nL 177.592211 160.049188 \nL 178.812105 159.22877 \nL 179.422052 159.51447 \nL 180.031999 159.111431 \nL 180.641946 159.053688 \nL 181.251894 159.335816 \nL 181.861841 159.277504 \nL 182.471788 159.891294 \nL 184.301629 159.711433 \nL 184.911577 160.310555 \nL 185.521524 160.249095 \nL 186.131471 160.513871 \nL 186.741418 160.128 \nL 188.57126 160.908471 \nL 189.181207 160.845881 \nL 189.791154 160.467803 \nL 190.401101 160.407965 \nL 191.620995 159.666928 \nL 192.230943 158.991678 \nL 192.84089 158.939999 \nL 193.450837 159.19593 \nL 194.060784 158.838119 \nL 194.670731 158.179438 \nL 195.280678 158.738132 \nL 195.890626 158.387447 \nL 196.500573 158.339998 \nL 197.720467 158.840919 \nL 198.330414 158.79206 \nL 198.940361 159.038181 \nL 201.38015 158.843575 \nL 201.990097 159.372007 \nL 202.600044 159.035574 \nL 203.209992 159.273043 \nL 205.64978 159.080262 \nL 206.259727 158.474486 \nL 206.869675 158.430126 \nL 207.479622 158.10923 \nL 208.699516 158.573903 \nL 209.309463 158.529872 \nL 209.91941 158.758487 \nL 210.529358 158.44293 \nL 211.139305 157.590001 \nL 211.749252 157.819667 \nL 212.359199 158.315211 \nL 212.969146 158.273333 \nL 213.579093 158.762957 \nL 214.189041 158.19061 \nL 214.798988 157.886342 \nL 219.678565 159.620319 \nL 220.288512 159.321178 \nL 221.508407 160.242722 \nL 222.118354 160.195814 \nL 222.728301 159.899077 \nL 224.558142 159.764434 \nL 225.16809 159.96662 \nL 226.387984 158.899251 \nL 229.43772 158.700001 \nL 230.047667 158.900149 \nL 231.267561 158.821318 \nL 231.877508 159.01883 \nL 232.487456 158.97927 \nL 233.70735 159.368878 \nL 234.317297 159.095402 \nL 234.927244 158.591612 \nL 235.537191 158.785716 \nL 236.147139 159.209043 \nL 236.757086 159.399574 \nL 237.367033 159.359792 \nL 237.97698 159.54845 \nL 238.586927 159.963163 \nL 239.196874 160.148393 \nL 239.806822 159.880769 \nL 240.416769 160.289998 \nL 241.636663 160.206213 \nL 242.24661 160.387426 \nL 242.856557 160.123568 \nL 243.466505 160.524985 \nL 244.076452 160.703263 \nL 244.686399 160.66068 \nL 245.296346 160.180538 \nL 245.906293 160.140002 \nL 246.51624 160.317179 \nL 247.126188 160.27646 \nL 248.956029 160.7996 \nL 249.565976 160.757819 \nL 250.175923 160.929473 \nL 250.785871 160.887547 \nL 252.005765 161.648793 \nL 253.835606 161.518071 \nL 254.445554 161.266688 \nL 255.055501 161.432307 \nL 256.275395 161.347643 \nL 256.885342 161.717148 \nL 257.495289 161.469115 \nL 258.105237 161.427064 \nL 259.325131 162.1563 \nL 259.935078 162.112501 \nL 260.545025 161.665237 \nL 261.154972 161.824474 \nL 261.76492 161.581491 \nL 264.204708 162.209727 \nL 264.814655 162.166835 \nL 265.424603 161.927232 \nL 266.03455 161.885461 \nL 266.644497 161.648156 \nL 267.254444 161.607475 \nL 267.864391 161.761623 \nL 269.084286 162.454032 \nL 270.30418 162.369079 \nL 270.914127 162.518698 \nL 272.134021 163.196469 \nL 273.353916 163.487373 \nL 273.963863 162.875862 \nL 274.57381 163.021396 \nL 275.183757 162.790436 \nL 275.793704 163.122658 \nL 276.403652 163.079487 \nL 277.623546 163.365216 \nL 278.233493 163.321716 \nL 278.84344 163.647691 \nL 280.063335 163.559832 \nL 280.673282 163.333226 \nL 281.283229 163.290421 \nL 281.893176 163.429891 \nL 282.503123 163.750083 \nL 283.723018 164.024128 \nL 284.332965 163.619998 \nL 284.942912 163.397621 \nL 286.162806 164.027605 \nL 286.772753 164.161977 \nL 287.382701 163.585478 \nL 287.992648 163.36623 \nL 289.212542 163.283478 \nL 289.822489 163.593656 \nL 290.432436 163.376755 \nL 291.042384 163.335688 \nL 292.262278 163.949121 \nL 293.482172 164.210401 \nL 294.092119 163.995325 \nL 294.702067 163.953263 \nL 295.312014 164.082856 \nL 297.141855 163.957329 \nL 297.751802 164.08555 \nL 298.36175 163.70554 \nL 298.971697 163.665004 \nL 299.581644 163.288055 \nL 300.191591 163.248813 \nL 300.801538 163.042331 \nL 301.411485 163.170929 \nL 302.021433 163.465454 \nL 303.241327 163.718518 \nL 303.851274 163.678774 \nL 304.461221 163.474357 \nL 305.071168 163.435434 \nL 305.681116 163.560759 \nL 306.291063 163.358186 \nL 306.90101 163.483076 \nL 307.510957 163.770148 \nL 308.120904 163.893387 \nL 308.730851 163.530004 \nL 309.340799 163.491622 \nL 309.950746 163.614629 \nL 310.560693 163.897819 \nL 311.17064 163.858809 \nL 312.390534 164.100594 \nL 313.000482 164.061381 \nL 313.610429 163.863531 \nL 314.220376 163.82509 \nL 315.44027 164.064085 \nL 316.050217 164.340004 \nL 316.660165 164.143872 \nL 317.880059 164.379043 \nL 318.490006 164.340004 \nL 319.099953 164.456551 \nL 319.7099 164.262486 \nL 320.319848 164.533316 \nL 321.539742 164.763282 \nL 322.149689 164.416778 \nL 322.759636 164.531491 \nL 323.369583 164.492828 \nL 323.979531 163.996947 \nL 325.199425 163.922673 \nL 325.809372 164.188598 \nL 327.029266 164.415349 \nL 327.639214 164.678281 \nL 328.249161 164.79 \nL 328.859108 165.050856 \nL 329.469055 165.161204 \nL 330.079002 165.420003 \nL 330.688949 165.528989 \nL 331.298897 165.489197 \nL 331.908844 165.30165 \nL 333.128738 165.223639 \nL 333.738685 165.037965 \nL 335.568527 164.923783 \nL 336.178474 165.031688 \nL 336.788421 164.703232 \nL 337.398368 164.666172 \nL 338.008315 164.773932 \nL 338.618263 164.736883 \nL 339.22821 164.267999 \nL 339.838157 164.375924 \nL 340.448104 164.196642 \nL 341.058051 164.304238 \nL 341.667998 164.125902 \nL 342.277946 163.805933 \nL 342.887893 164.05579 \nL 344.107787 164.269264 \nL 344.717734 164.516476 \nL 345.937629 164.164294 \nL 346.547576 164.269872 \nL 347.76747 164.20035 \nL 348.377417 164.305165 \nL 348.987365 164.270471 \nL 350.207259 163.924614 \nL 350.817206 164.02913 \nL 351.427153 163.857447 \nL 352.0371 164.099242 \nL 352.647048 163.79085 \nL 353.256995 163.894758 \nL 353.866942 163.861522 \nL 355.086836 164.340004 \nL 355.696783 164.306044 \nL 356.306731 164.407788 \nL 356.916678 164.644388 \nL 358.136572 164.845203 \nL 358.746519 164.810538 \nL 359.356466 164.641868 \nL 359.966414 164.741653 \nL 360.576361 164.707422 \nL 361.186308 164.806666 \nL 361.796255 165.038563 \nL 362.406202 165.136726 \nL 363.016149 164.969446 \nL 363.626097 165.067348 \nL 364.845991 164.735125 \nL 365.455938 164.832901 \nL 366.065885 164.536763 \nL 366.675832 164.372728 \nL 367.28578 164.470649 \nL 367.895727 164.046646 \nL 368.505674 164.144818 \nL 369.115621 164.372467 \nL 369.725568 164.210401 \nL 369.725568 164.210401 \n\" clip-path=\"url(#p723db9eccb)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 50.14375 169.524002 \nL 384.94375 169.524002 \n\" clip-path=\"url(#p723db9eccb)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #000000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 256.68 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 256.68 \nL 384.94375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 256.68 \nL 384.94375 256.68 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 384.94375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 302.089063 103.26875 \nL 377.94375 103.26875 \nQ 379.94375 103.26875 379.94375 101.26875 \nL 379.94375 14.2 \nQ 379.94375 12.2 377.94375 12.2 \nL 302.089063 12.2 \nQ 300.089063 12.2 300.089063 14.2 \nL 300.089063 101.26875 \nQ 300.089063 103.26875 302.089063 103.26875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 304.089063 20.298437 \nL 314.089063 20.298437 \nL 324.089063 20.298437 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- P(die=1) -->\n     <g transform=\"translate(332.089063 23.798437) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 304.089063 34.976562 \nL 314.089063 34.976562 \nL 324.089063 34.976562 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- P(die=2) -->\n     <g transform=\"translate(332.089063 38.476562) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-32\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 304.089063 49.654687 \nL 314.089063 49.654687 \nL 324.089063 49.654687 \n\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- P(die=3) -->\n     <g transform=\"translate(332.089063 53.154687) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-33\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_25\">\n     <path d=\"M 304.089063 64.332812 \nL 314.089063 64.332812 \nL 324.089063 64.332812 \n\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_20\">\n     <!-- P(die=4) -->\n     <g transform=\"translate(332.089063 67.832812) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-34\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_26\">\n     <path d=\"M 304.089063 79.010937 \nL 314.089063 79.010937 \nL 324.089063 79.010937 \n\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_21\">\n     <!-- P(die=5) -->\n     <g transform=\"translate(332.089063 82.510937) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-35\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n    <g id=\"line2d_27\">\n     <path d=\"M 304.089063 93.689062 \nL 314.089063 93.689062 \nL 324.089063 93.689062 \n\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_22\">\n     <!-- P(die=6) -->\n     <g transform=\"translate(332.089063 97.189062) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n      <use xlink:href=\"#DejaVuSans-36\" x=\"335.888672\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p723db9eccb\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"334.8\" height=\"249.48\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.1.1 概率论公理"
      ],
      "metadata": {
        "id": "Ik0GwOkQrc-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在处理骰子掷出时，我们将集合S成为样本空间，其中每个元素都是结果。\n",
        "\n",
        "事件（event）是一组给定样本空间的随机结果。\n",
        "\n",
        "概率（probability）可以被认为是将集合映射到真实值的函数。\n",
        "\n",
        "以上也是概率论的公理，由科尔莫戈罗夫于1933年提出。 有了这个公理系统，我们可以避免任何关于随机性的哲学争论； 相反，我们可以用数学语言严格地推理。"
      ],
      "metadata": {
        "id": "-rLU05J8rjA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.1.2 随机变量"
      ],
      "metadata": {
        "id": "MqJ7xQ5zrxy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在我们掷骰子的随机实验中，我们引入了随机变量（random variable）的概念。 随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。"
      ],
      "metadata": {
        "id": "uHKfFYUwr1JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "请注意，离散（discrete）随机变量（如骰子的每一面） 和连续（continuous）随机变量（如人的体重和身高）之间存在微妙的区别。实生活中，测量两个人是否具有完全相同的身高没有太大意义。 如果我们进行足够精确的测量，最终会发现这个星球上没有两个人具有完全相同的身高。 在这种情况下，询问某人的身高是否落入给定的区间，比如是否在1.79米和1.81米之间更有意义。 在这些情况下，我们将这个看到某个数值的可能性量化为密度（density）。"
      ],
      "metadata": {
        "id": "VN2-Sy4tr8uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6.2 处理多个随机变量"
      ],
      "metadata": {
        "id": "E4dDnDassDAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "很多时候，我们会考虑多个随机变量。 我们需要估计这些概率以及概率之间的关系，以便我们可以运用我们的推断来实现更好的医疗服务。\n",
        "\n",
        "所有这些都是联合发生的随机变量。 当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。"
      ],
      "metadata": {
        "id": "_l6xHLNAsI1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2.1 联合概率"
      ],
      "metadata": {
        "id": "CFlnkXALsOvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "第一个被称为联合概率（joint probability）"
      ],
      "metadata": {
        "id": "9io9zmJOsSKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2.2 条件概率"
      ],
      "metadata": {
        "id": "-ok8GR-ZsUvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2.3 贝叶斯定理"
      ],
      "metadata": {
        "id": "N1lyQgtpsX7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2.4 边际化(边缘概率)"
      ],
      "metadata": {
        "id": "vllVL2eEsbN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2.5 独立性"
      ],
      "metadata": {
        "id": "MsQTHFw3siqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6.3 期望和方差"
      ],
      "metadata": {
        "id": "m8ir3Qu0smJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了概括概率分布的关键特征，我们需要一些测量方法。后续略"
      ],
      "metadata": {
        "id": "BYhRjpIisr6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6.4 小结\n"
      ],
      "metadata": {
        "id": "C7h2tZ1OsxXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以从概率分布中采样。\n",
        "\n",
        "我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。\n",
        "\n",
        "期望和方差为概率分布的关键特征的概括提供了实用的度量形式。"
      ],
      "metadata": {
        "id": "eLb6ajbbs2ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.7 查阅文档"
      ],
      "metadata": {
        "id": "0rzoOryws94D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以从概率分布中采样。\n",
        "\n",
        "我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。\n",
        "\n",
        "期望和方差为概率分布的关键特征的概括提供了实用的度量形式。"
      ],
      "metadata": {
        "id": "dx3pDDTatBdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7.1 查找模块中的所有函数和类"
      ],
      "metadata": {
        "id": "CYGRbnePtDvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以从概率分布中采样。\n",
        "\n",
        "我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。\n",
        "\n",
        "期望和方差为概率分布的关键特征的概括提供了实用的度量形式。"
      ],
      "metadata": {
        "id": "VTAIlBGFtHtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(dir(torch.distributions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRba8LTwtJfH",
        "outputId": "2137c52e-28e2-4a47-9716-02a7303ccc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PositiveDefiniteTransform', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通常可以忽略以“__”（双下划线）开始和结束的函数，它们是Python中的特殊对象， 或以单个“_”（单下划线）开始的函数，它们通常是内部函数。 根据剩余的函数名或属性名，我们可能会猜测这个模块提供了各种生成随机数的方法， 包括从均匀分布（uniform）、正态分布（normal）和多项分布（multinomial）中采样。"
      ],
      "metadata": {
        "id": "1y9QuyzotOfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7.2 查找特定函数和类的用法"
      ],
      "metadata": {
        "id": "7VhRLWKBtQft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "有关如何使用给定函数或类的更具体说明，可以调用help函数。 例如，我们来查看张量ones函数的用法。"
      ],
      "metadata": {
        "id": "RyIiCU8CtU3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(torch.ones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVtnHi6LtXDh",
        "outputId": "e4be8e15-aee4-4d29-be1b-3f80493be90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function ones in module torch:\n",
            "\n",
            "ones(...)\n",
            "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
            "    \n",
            "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
            "    by the variable argument :attr:`size`.\n",
            "    \n",
            "    Args:\n",
            "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
            "            Can be a variable number of arguments or a collection like a list or tuple.\n",
            "    \n",
            "    Keyword arguments:\n",
            "        out (Tensor, optional): the output tensor.\n",
            "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
            "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
            "            Default: ``torch.strided``.\n",
            "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            "            Default: if ``None``, uses the current device for the default tensor type\n",
            "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
            "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
            "        requires_grad (bool, optional): If autograd should record operations on the\n",
            "            returned tensor. Default: ``False``.\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> torch.ones(2, 3)\n",
            "        tensor([[ 1.,  1.,  1.],\n",
            "                [ 1.,  1.,  1.]])\n",
            "    \n",
            "        >>> torch.ones(5)\n",
            "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "从文档中，我们可以看到ones函数创建一个具有指定形状的新张量，并将所有元素值设置为1。 下面来运行一个快速测试来确认这一解释："
      ],
      "metadata": {
        "id": "vp8bvmOPtap_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr3KhaCItdXa",
        "outputId": "12a4a2ae-f0ed-4bfe-cd65-7cf11c9ee0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在Jupyter记事本中，我们可以使用?指令在另一个浏览器窗口中显示文档。 例如，list?指令将创建与help(list)指令几乎相同的内容，并在新的浏览器窗口中显示它。 此外，如果我们使用两个问号，如list??，将显示实现该函数的Python代码。"
      ],
      "metadata": {
        "id": "s1jVW2wlthzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list??"
      ],
      "metadata": {
        "id": "g2rNkP0AtjqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7.3 小结"
      ],
      "metadata": {
        "id": "LKUm3c09tpCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "官方文档提供了本书之外的大量描述和示例。\n",
        "\n",
        "可以通过调用dir和help函数或在Jupyter记事本中使用?和??查看API的用法文档。"
      ],
      "metadata": {
        "id": "Jw1Jve5Ctri9"
      }
    }
  ]
}